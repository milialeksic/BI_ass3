{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler # we had to manually instal scikit-learn because we need to use it for ML and it is not in requirements.txt\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12332263'  # Replace the digits after \"id_\" with your own student ID\n",
    "#executed_by ='stud-id_12424821'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '008'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12332263'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12424821'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4080a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Milica\" .',\n",
    "f':{student_a} foaf:familyName \"Aleksic\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12424821\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Vidak\" .',\n",
    "f':{student_b} foaf:familyName \"Grujic\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12332263\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "supermarket_data_path = os.path.join(\"dataset_supermarket_analysis\", \"supermarket_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The dataset is the Supermarket Sales dataset. It contains transaction-level data including\n",
    "branch, customer type, gender, product line, payment method, unit price, quantity, tax, total sales, date, time,\n",
    "and a customer satisfaction rating (4 to 10). Scenario: The supermarket chain wants to monitor customer satisfaction\n",
    "and predict the rating a customer is likely to give. The business particularly wants to detect low predicted ratings\n",
    "early to improve service quality and customer experience.\n",
    "\"\"\"\n",
    "#business_objectives_comment =\"\"\"...\"\"\"\n",
    "#business_success_criteria_comment = \"\"\"...\"\"\"\n",
    "#data_mining_goals_comment = \"\"\"...\"\"\"\n",
    "#data_mining_success_criteria_comment = \"\"\"...\"\"\"\n",
    "#ai_risk_aspects_comment = \"\"\"...\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The primary business objectives are:\n",
    "1. Understand which factors affects customer satisfaction.\n",
    "2. Predict customer ratings based on available information.\n",
    "3. Detect scenarios likely to produce low ratings and support decisions to improve service.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "The project will be considered a business success if:\n",
    "1. The model predicts customer ratings with useful accuracy (e.g., MAE sufficiently low to distinguish low vs. high satisfaction cases).\n",
    "2. The system can identify transactions likely to result in low ratings, which would enable proactive service improvements.\n",
    "3. The analysis provides useful insights into factors associated with customer satisfaction.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The data mining goals are:\n",
    "1. Train a regression model to predict the continuous customer rating variable.\n",
    "2. Understand feature importance to identify which attributes most influence satisfaction.\n",
    "3. Optionally compare multiple ML models.\n",
    "4. Ensure full reproducibility of all experiments.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "Data mining will be considered successful if:\n",
    "1. The model achieves meaningful and realistic MAE threshold (e.g., ≤ 1.0).\n",
    "2. The model is interpretable, allowing business users to understand what determines the low satisfaction.\n",
    "3. The entire workflow is reproducible and computationally efficient.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Relevant AI risks include:\n",
    "1. Customer type (Member/Normal) could result in unfair treatment if misused.\n",
    "2. Using model predictions to treat customers differently could be unethical or discriminatory.\n",
    "3. Predictions must be used only as decision support and not as a mechanism for unequal service.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"b976c7cd-08be-4134-9783-22f1c3441a13\" \n",
    "#bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# uu\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice ID</th>\n",
       "      <th>Branch</th>\n",
       "      <th>City</th>\n",
       "      <th>Customer type</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Product line</th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Tax 5%</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Payment</th>\n",
       "      <th>cogs</th>\n",
       "      <th>gross margin percentage</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750-67-8428</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>74.69</td>\n",
       "      <td>7</td>\n",
       "      <td>26.1415</td>\n",
       "      <td>548.9715</td>\n",
       "      <td>1/5/2019</td>\n",
       "      <td>1:08:00 PM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>522.83</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>26.1415</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226-31-3081</td>\n",
       "      <td>Giza</td>\n",
       "      <td>Naypyitaw</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Electronic accessories</td>\n",
       "      <td>15.28</td>\n",
       "      <td>5</td>\n",
       "      <td>3.8200</td>\n",
       "      <td>80.2200</td>\n",
       "      <td>3/8/2019</td>\n",
       "      <td>10:29:00 AM</td>\n",
       "      <td>Cash</td>\n",
       "      <td>76.40</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>3.8200</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>631-41-3108</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Home and lifestyle</td>\n",
       "      <td>46.33</td>\n",
       "      <td>7</td>\n",
       "      <td>16.2155</td>\n",
       "      <td>340.5255</td>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>1:23:00 PM</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>324.31</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>16.2155</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123-19-1176</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>58.22</td>\n",
       "      <td>8</td>\n",
       "      <td>23.2880</td>\n",
       "      <td>489.0480</td>\n",
       "      <td>1/27/2019</td>\n",
       "      <td>8:33:00 PM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>465.76</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>23.2880</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373-73-7910</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Sports and travel</td>\n",
       "      <td>86.31</td>\n",
       "      <td>7</td>\n",
       "      <td>30.2085</td>\n",
       "      <td>634.3785</td>\n",
       "      <td>2/8/2019</td>\n",
       "      <td>10:37:00 AM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>604.17</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>30.2085</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Invoice ID Branch       City Customer type  Gender  \\\n",
       "0  750-67-8428   Alex     Yangon        Member  Female   \n",
       "1  226-31-3081   Giza  Naypyitaw        Normal  Female   \n",
       "2  631-41-3108   Alex     Yangon        Normal  Female   \n",
       "3  123-19-1176   Alex     Yangon        Member  Female   \n",
       "4  373-73-7910   Alex     Yangon        Member  Female   \n",
       "\n",
       "             Product line  Unit price  Quantity   Tax 5%     Sales       Date  \\\n",
       "0       Health and beauty       74.69         7  26.1415  548.9715   1/5/2019   \n",
       "1  Electronic accessories       15.28         5   3.8200   80.2200   3/8/2019   \n",
       "2      Home and lifestyle       46.33         7  16.2155  340.5255   3/3/2019   \n",
       "3       Health and beauty       58.22         8  23.2880  489.0480  1/27/2019   \n",
       "4       Sports and travel       86.31         7  30.2085  634.3785   2/8/2019   \n",
       "\n",
       "          Time      Payment    cogs  gross margin percentage  gross income  \\\n",
       "0   1:08:00 PM      Ewallet  522.83                 4.761905       26.1415   \n",
       "1  10:29:00 AM         Cash   76.40                 4.761905        3.8200   \n",
       "2   1:23:00 PM  Credit card  324.31                 4.761905       16.2155   \n",
       "3   8:33:00 PM      Ewallet  465.76                 4.761905       23.2880   \n",
       "4  10:37:00 AM      Ewallet  604.17                 4.761905       30.2085   \n",
       "\n",
       "   Rating  \n",
       "0     9.1  \n",
       "1     9.6  \n",
       "2     7.4  \n",
       "3     8.4  \n",
       "4     5.3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_supermarket_data_code_writer = student_a\n",
    "\n",
    "def load_supermarket_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Supermarket Sales dataset for Data Understanding.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(supermarket_data_path, sep=',', header=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_supermarket_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"2434d382-04c0-4aa2-aaf7-23a082327305\"  # Generate once\n",
    "load_supermarket_data_executor = [\n",
    "    f':load_supermarket_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "engine.insert(load_supermarket_data_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "ld_ass_uuid_writer = \"c3662868-04b8-4958-a7ba-d6ccc2849f9f\" \n",
    "\n",
    "ld_report = \"\"\"\n",
    "Load the Supermarket Sales (Extended Version) dataset into a DataFrame for initial Data Understanding, exploration, and quality assessment.\n",
    "\"\"\"\n",
    "\n",
    "load_supermarket_data_activity = [\n",
    "    ':load_supermarket_data rdf:type prov:Activity .',\n",
    "    ':load_supermarket_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_supermarket_data rdfs:comment \"Data Understanding\" .',\n",
    "    f':load_supermarket_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_supermarket_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_supermarket_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_supermarket_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_supermarket_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # INPUT\n",
    "    ':load_supermarket_data prov:used :raw_data .',\n",
    "    ':load_supermarket_data prov:used :supermarket_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':supermarket_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :supermarket_data_path .',\n",
    "\n",
    "    # OUTPUT\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_supermarket_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_supermarket_data_activity, prefixes=prefixes)\n",
    "\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \"Supermarket Sales Extended Dataset\" .',\n",
    "    ':raw_data sc:description \"Transaction-level supermarket sales data including customer attributes, product information, sales amounts, timestamps, and customer ratings.\" .',\n",
    "\n",
    "    ':supermarket_csv rdf:type cr:FileObject .',\n",
    "    ':supermarket_csv sc:name \"supermarket_analysis.csv\" .',\n",
    "    ':supermarket_csv sc:encodingFormat \"text/csv\" .',\n",
    "    ':raw_data sc:distribution :supermarket_csv .',\n",
    "\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \"Supermarket Transaction Table\" .',\n",
    "    ':raw_recordset cr:source :supermarket_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "supermarket_fields = [\n",
    "    # Identifier\n",
    "    ('invoice_id', 'Invoice ID', 'Unique identifier for each transaction.', 'xsd:string'),\n",
    "\n",
    "    # Categorical\n",
    "    ('branch', 'Branch', 'Supermarket branch where the transaction occurred.', 'xsd:string'),\n",
    "    ('city', 'City', 'City where the branch is located.', 'xsd:string'),\n",
    "    ('customer_type', 'Customer type', 'Customer classification: Member or Normal.', 'xsd:string'),\n",
    "    ('gender', 'Gender', 'Gender of the customer.', 'xsd:string'),\n",
    "    ('product_line', 'Product line', 'Category of product purchased.', 'xsd:string'),\n",
    "    ('payment', 'Payment', 'Payment method used in the transaction.', 'xsd:string'),\n",
    "\n",
    "    # Numeric (monetary & counts)\n",
    "    ('unit_price', 'Unit price', 'Price per unit of product.', 'xsd:float'),\n",
    "    ('quantity', 'Quantity', 'Number of items purchased.', 'xsd:integer'),\n",
    "    ('tax_5', 'Tax 5%', '5 percent tax applied to the purchase.', 'xsd:float'),\n",
    "    ('sales', 'Sales', 'Total amount paid including tax.', 'xsd:float'),\n",
    "    ('cogs', 'cogs', 'Cost of goods sold (pre-tax).', 'xsd:float'),\n",
    "    ('gross_margin_percentage', 'gross margin percentage', 'Gross margin percentage (constant in dataset).', 'xsd:float'),\n",
    "    ('gross_income', 'gross income', 'Gross income from the transaction.', 'xsd:float'),\n",
    "\n",
    "    # Temporal\n",
    "    ('date', 'Date', 'Date of transaction.', 'xsd:date'),\n",
    "    ('time', 'Time', 'Time of transaction.', 'xsd:string'),\n",
    "\n",
    "    # Target\n",
    "    ('rating', 'Rating', 'Customer satisfaction rating on a scale from 4 to 10.', 'xsd:float'),\n",
    "]\n",
    "\n",
    "field_triples = []\n",
    "for f_id, name, desc, dtype in supermarket_fields:\n",
    "    field_triples.extend([\n",
    "        f':raw_recordset cr:field :field_{f_id} .',\n",
    "        f':field_{f_id} rdf:type cr:Field .',\n",
    "        f':field_{f_id} sc:name \"{name}\" .',\n",
    "        f':field_{f_id} sc:description \"{desc}\" .',\n",
    "        f':field_{f_id} cr:dataType {dtype} .',\n",
    "    ])\n",
    "\n",
    "engine.insert(field_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':data sc:name \"Loaded Supermarket Sales Dataset\" .',\n",
    "    ':data sc:description \"Supermarket transaction data loaded for Data Understanding and exploratory analysis.\" .',\n",
    "\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':recordset sc:name \"Supermarket Sales RecordSet\" .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "]\n",
    "\n",
    "for fid, *_ in supermarket_fields:\n",
    "    data_triples.append(f':recordset cr:field :field_{fid} .')\n",
    "\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "units_triples = [\n",
    "\n",
    "    # Monetary values (currency unspecified in dataset)\n",
    "    ':field_unit_price qudt:unit qudt:CurrencyUnit .',\n",
    "    ':field_tax_5 qudt:unit qudt:CurrencyUnit .',\n",
    "    ':field_sales qudt:unit qudt:CurrencyUnit .',\n",
    "    ':field_cogs qudt:unit qudt:CurrencyUnit .',\n",
    "    ':field_gross_income qudt:unit qudt:CurrencyUnit .',\n",
    "\n",
    "    # Percentage\n",
    "    ':field_gross_margin_percentage qudt:unit qudt:Percent .',\n",
    "\n",
    "    # Counts\n",
    "    ':field_quantity qudt:unit qudt:CountUnit .',\n",
    "\n",
    "    # Rating (dimensionless score)\n",
    "    ':field_rating qudt:unit qudt:DimensionlessUnit .',\n",
    "\n",
    "    # Temporal\n",
    "    ':field_date qudt:unit siu:day .',\n",
    "    ':field_time qudt:unit siu:hour .',\n",
    "]\n",
    "\n",
    "engine.insert(units_triples, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unit price': [], 'Quantity': [], 'Tax 5%': [{'index': 49, 'z_score': 2.2150497538876905}, {'index': 70, 'z_score': 2.50756414873952}, {'index': 96, 'z_score': 2.438812590241499}, {'index': 105, 'z_score': 2.472974855333683}, {'index': 122, 'z_score': 2.5282323191202916}, {'index': 140, 'z_score': 2.5212290547763936}, {'index': 141, 'z_score': 2.5511210367320554}, {'index': 158, 'z_score': 2.4229271369736334}, {'index': 166, 'z_score': 2.7680514200674264}, {'index': 167, 'z_score': 2.9132410467092105}, {'index': 209, 'z_score': 2.519392833027689}, {'index': 211, 'z_score': 2.2707342459879514}, {'index': 350, 'z_score': 2.926905952746084}, {'index': 357, 'z_score': 2.762073023676294}, {'index': 422, 'z_score': 2.837657035192752}, {'index': 429, 'z_score': 2.55752646143684}, {'index': 435, 'z_score': 2.500560884395622}, {'index': 529, 'z_score': 2.456363453932609}, {'index': 557, 'z_score': 2.893597744281204}, {'index': 561, 'z_score': 2.495607355957256}, {'index': 611, 'z_score': 2.490184096373871}, {'index': 623, 'z_score': 2.2415255093341337}, {'index': 627, 'z_score': 2.2129146123194294}, {'index': 663, 'z_score': 2.4059314100902713}, {'index': 699, 'z_score': 2.8500408562886688}, {'index': 739, 'z_score': 2.2976370297480466}, {'index': 792, 'z_score': 2.844489488211189}, {'index': 906, 'z_score': 2.2996013599908474}, {'index': 941, 'z_score': 2.5228517623682722}, {'index': 959, 'z_score': 2.4782700064229717}, {'index': 970, 'z_score': 2.300455416618152}, {'index': 982, 'z_score': 2.432919599513097}, {'index': 988, 'z_score': 2.202665932791774}, {'index': 996, 'z_score': 2.844916516524841}], 'Sales': [{'index': 49, 'z_score': 2.2150497538876905}, {'index': 70, 'z_score': 2.5075641487395197}, {'index': 96, 'z_score': 2.4388125902414983}, {'index': 105, 'z_score': 2.4729748553336823}, {'index': 122, 'z_score': 2.5282323191202907}, {'index': 140, 'z_score': 2.5212290547763927}, {'index': 141, 'z_score': 2.551121036732055}, {'index': 158, 'z_score': 2.422927136973633}, {'index': 166, 'z_score': 2.7680514200674255}, {'index': 167, 'z_score': 2.9132410467092096}, {'index': 209, 'z_score': 2.5193928330276885}, {'index': 211, 'z_score': 2.2707342459879514}, {'index': 350, 'z_score': 2.926905952746084}, {'index': 357, 'z_score': 2.762073023676293}, {'index': 422, 'z_score': 2.8376570351927515}, {'index': 429, 'z_score': 2.5575264614368396}, {'index': 435, 'z_score': 2.5005608843956217}, {'index': 529, 'z_score': 2.456363453932608}, {'index': 557, 'z_score': 2.8935977442812035}, {'index': 561, 'z_score': 2.495607355957255}, {'index': 611, 'z_score': 2.490184096373871}, {'index': 623, 'z_score': 2.2415255093341333}, {'index': 627, 'z_score': 2.2129146123194285}, {'index': 663, 'z_score': 2.405931410090271}, {'index': 699, 'z_score': 2.8500408562886683}, {'index': 739, 'z_score': 2.2976370297480466}, {'index': 792, 'z_score': 2.8444894882111886}, {'index': 906, 'z_score': 2.299601359990847}, {'index': 941, 'z_score': 2.5228517623682722}, {'index': 959, 'z_score': 2.4782700064229717}, {'index': 970, 'z_score': 2.3004554166181514}, {'index': 982, 'z_score': 2.4329195995130966}, {'index': 988, 'z_score': 2.2026659327917733}, {'index': 996, 'z_score': 2.8449165165248407}], 'cogs': [{'index': 49, 'z_score': 2.2150497538876905}, {'index': 70, 'z_score': 2.5075641487395197}, {'index': 96, 'z_score': 2.4388125902414988}, {'index': 105, 'z_score': 2.472974855333683}, {'index': 122, 'z_score': 2.528232319120291}, {'index': 140, 'z_score': 2.5212290547763936}, {'index': 141, 'z_score': 2.551121036732055}, {'index': 158, 'z_score': 2.422927136973633}, {'index': 166, 'z_score': 2.768051420067426}, {'index': 167, 'z_score': 2.9132410467092096}, {'index': 209, 'z_score': 2.5193928330276885}, {'index': 211, 'z_score': 2.270734245987951}, {'index': 350, 'z_score': 2.926905952746084}, {'index': 357, 'z_score': 2.7620730236762934}, {'index': 422, 'z_score': 2.8376570351927515}, {'index': 429, 'z_score': 2.5575264614368396}, {'index': 435, 'z_score': 2.5005608843956213}, {'index': 529, 'z_score': 2.4563634539326085}, {'index': 557, 'z_score': 2.893597744281204}, {'index': 561, 'z_score': 2.4956073559572554}, {'index': 611, 'z_score': 2.4901840963738713}, {'index': 623, 'z_score': 2.2415255093341337}, {'index': 627, 'z_score': 2.212914612319429}, {'index': 663, 'z_score': 2.4059314100902713}, {'index': 699, 'z_score': 2.8500408562886688}, {'index': 739, 'z_score': 2.297637029748046}, {'index': 792, 'z_score': 2.844489488211189}, {'index': 906, 'z_score': 2.299601359990847}, {'index': 941, 'z_score': 2.522851762368272}, {'index': 959, 'z_score': 2.478270006422972}, {'index': 970, 'z_score': 2.3004554166181514}, {'index': 982, 'z_score': 2.4329195995130966}, {'index': 988, 'z_score': 2.2026659327917733}, {'index': 996, 'z_score': 2.8449165165248407}], 'gross income': [{'index': 49, 'z_score': 2.2150497538876905}, {'index': 70, 'z_score': 2.50756414873952}, {'index': 96, 'z_score': 2.438812590241499}, {'index': 105, 'z_score': 2.472974855333683}, {'index': 122, 'z_score': 2.5282323191202916}, {'index': 140, 'z_score': 2.5212290547763936}, {'index': 141, 'z_score': 2.5511210367320554}, {'index': 158, 'z_score': 2.4229271369736334}, {'index': 166, 'z_score': 2.7680514200674264}, {'index': 167, 'z_score': 2.9132410467092105}, {'index': 209, 'z_score': 2.519392833027689}, {'index': 211, 'z_score': 2.2707342459879514}, {'index': 350, 'z_score': 2.926905952746084}, {'index': 357, 'z_score': 2.762073023676294}, {'index': 422, 'z_score': 2.837657035192752}, {'index': 429, 'z_score': 2.55752646143684}, {'index': 435, 'z_score': 2.500560884395622}, {'index': 529, 'z_score': 2.456363453932609}, {'index': 557, 'z_score': 2.893597744281204}, {'index': 561, 'z_score': 2.495607355957256}, {'index': 611, 'z_score': 2.490184096373871}, {'index': 623, 'z_score': 2.2415255093341337}, {'index': 627, 'z_score': 2.2129146123194294}, {'index': 663, 'z_score': 2.4059314100902713}, {'index': 699, 'z_score': 2.8500408562886688}, {'index': 739, 'z_score': 2.2976370297480466}, {'index': 792, 'z_score': 2.844489488211189}, {'index': 906, 'z_score': 2.2996013599908474}, {'index': 941, 'z_score': 2.5228517623682722}, {'index': 959, 'z_score': 2.4782700064229717}, {'index': 970, 'z_score': 2.300455416618152}, {'index': 982, 'z_score': 2.432919599513097}, {'index': 988, 'z_score': 2.202665932791774}, {'index': 996, 'z_score': 2.844916516524841}], 'Rating': []}\n"
     ]
    }
   ],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "\n",
    "# We don't include the gross margin percentage column as it is constant.\n",
    "columns =  (\n",
    "        'Unit price',\n",
    "        'Quantity',\n",
    "        'Tax 5%',\n",
    "        'Sales',\n",
    "        'cogs',\n",
    "        'gross income',\n",
    "        'Rating'\n",
    "    )\n",
    "\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, threshold=3.0, columns=columns) -> dict:\n",
    "    results = {}\n",
    "    df = data.reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        values = df[col].astype(float)\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        # Skip columns where Z-scores are undefined (e.g. constant column)\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "        mask = z_scores.abs() > threshold\n",
    "\n",
    "        outliers = [\n",
    "            {\n",
    "                \"index\": int(idx),\n",
    "                \"z_score\": float(z_scores.iloc[idx])\n",
    "            }\n",
    "            for idx in np.where(mask)[0]\n",
    "        ]\n",
    "\n",
    "        results[col] = outliers\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "start_time_co = now()\n",
    "outliers_report = check_outliers(data, threshold=2.2)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => in this case a report\n",
    "# 2. activity inspects the outcome and derives decisions => in this case to remove the outliers that were found\n",
    "# 3. activity follows up on the decision by changing the data => will be done in the data preparation phase\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "#co_ass_uuid_executor = \"15085e9d-15f1-4727-9b6e-776dd07fcd08\"\n",
    "co_ass_uuid_executor = \"4803f9b0-c2b7-427b-be61-a37f2279bc79\"\n",
    "check_outliers_executor = [\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "engine.insert(check_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"e44e7b7b-89d5-44f1-abbe-5deb0e06f190\"\n",
    "#co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "\n",
    "co_comment = \"\"\"\n",
    "Potential outliers were identified using Z-score based approach for numerical attributes.\n",
    "Treshold of 2.2 is used as cut-off value.\n",
    "The analysis shows that outliers were not detected for Unit price, Quantity, or Rating.\n",
    "\n",
    "Outliers were identified for Tax 5%, Sales, cogs, and gross income. These outliers occur\n",
    "at the same transaction indices across these attributes, which is expected because\n",
    "these monetary variables are deterministically related (Sales = cogs + Tax 5% and\n",
    "gross income is proportional to cogs).\n",
    "\n",
    "The detected outliers correspond to unusually big purchase rather than\n",
    "error values. This indicates the presence of high-value transactions rather than\n",
    "data quality issues.\n",
    "\"\"\"\n",
    "\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:comment \"Data Understanding – Outlier Detection\" .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{check_outliers_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_outliers prov:used :data .',\n",
    "\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outliers_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "]\n",
    "engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ior_ass_uuid_executor = \"67d200d8-99e2-4ad9-8821-131ada033f02\"\n",
    "\n",
    "ior_comment = \"\"\"\n",
    "The outlier report was inspected to check the authenticity of the detected values.\n",
    "The identified outliers occur only in monetary attributes and correspond to\n",
    "large but realistic purchase amounts.\n",
    "\n",
    "Because these values represent valid high-spending transactions and no outliers were\n",
    "detected for the target variable (Rating), no records are removed during the Data\n",
    "Understanding phase. The presence of these values reflects natural variability in\n",
    "customer spending rather than data errors.\n",
    "\n",
    "Any potential handling of extreme values (e.g. capping or transformation) is moved\n",
    "to the Data Preparation phase if required by the modeling approach.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inspect_outlier_report_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':inspect_outlier_report rdfs:comment \"Data Understanding – Outlier Assessment\" .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{ior_comment}\"\"\" .',\n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{student_a} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    f':outlier_decision rdfs:comment \"No outliers removed during Data Understanding; decision deferred to Data Preparation.\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "]\n",
    "engine.insert(inspect_outlier_report_activity, prefixes=prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94dfb7-328c-432b-b7e7-1f66f03eabca",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Understanding phase such as checking the distribution, skewness, plausibility of values, etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Rating:\n",
      " Rating                     1.000000\n",
      "Unit price                -0.008778\n",
      "Quantity                  -0.015815\n",
      "cogs                      -0.036442\n",
      "Tax 5%                    -0.036442\n",
      "gross income              -0.036442\n",
      "Sales                     -0.036442\n",
      "gross margin percentage         NaN\n",
      "dtype: float64\n",
      "\n",
      "Descriptive Statistics:\n",
      "                           count        mean         std        min  \\\n",
      "Unit price               1000.0   55.672130   26.494628  10.080000   \n",
      "Quantity                 1000.0    5.510000    2.923431   1.000000   \n",
      "Tax 5%                   1000.0   15.379369   11.708825   0.508500   \n",
      "Sales                    1000.0  322.966749  245.885335  10.678500   \n",
      "cogs                     1000.0  307.587380  234.176510  10.170000   \n",
      "gross margin percentage  1000.0    4.761905    0.000000   4.761905   \n",
      "gross income             1000.0   15.379369   11.708825   0.508500   \n",
      "Rating                   1000.0    6.972700    1.718580   4.000000   \n",
      "\n",
      "                                25%         50%         75%          max  \n",
      "Unit price                32.875000   55.230000   77.935000    99.960000  \n",
      "Quantity                   3.000000    5.000000    8.000000    10.000000  \n",
      "Tax 5%                     5.924875   12.088000   22.445250    49.650000  \n",
      "Sales                    124.422375  253.848000  471.350250  1042.650000  \n",
      "cogs                     118.497500  241.760000  448.905000   993.000000  \n",
      "gross margin percentage    4.761905    4.761905    4.761905     4.761905  \n",
      "gross income               5.924875   12.088000   22.445250    49.650000  \n",
      "Rating                     5.500000    7.000000    8.500000    10.000000  \n"
     ]
    }
   ],
   "source": [
    "stats_code_writer = student_a\n",
    "\n",
    "\n",
    "def calculate_supermarket_stats(\n",
    "    data: pd.DataFrame,\n",
    "    target_col: str = 'Rating',\n",
    "    numeric_cols = ('Unit price', 'Quantity', 'Tax 5%', 'Sales', 'cogs', 'gross margin percentage', 'gross income', 'Rating')\n",
    ") -> dict:\n",
    "    # Basic checks\n",
    "    if target_col not in data.columns:\n",
    "        return {'error': f'{target_col} column not found for correlation.'}\n",
    "\n",
    "    \n",
    "    numeric_cols = [c for c in numeric_cols if c in data.columns]\n",
    "    df_num = data[numeric_cols].copy()\n",
    "\n",
    "   \n",
    "    for c in numeric_cols:\n",
    "        df_num[c] = pd.to_numeric(df_num[c], errors='coerce')\n",
    "\n",
    "   \n",
    "    correlation_matrix = df_num.corr(numeric_only=True)\n",
    "\n",
    "  \n",
    "    if target_col not in correlation_matrix.columns:\n",
    "        return {'error': f'Could not compute correlations for target {target_col}.'}\n",
    "\n",
    "    target_correlation = correlation_matrix[target_col].sort_values(ascending=False).to_dict()\n",
    "\n",
    "    descriptive_stats = df_num.describe().T.to_dict()\n",
    "\n",
    "    return {\n",
    "        'correlation_with_target': target_correlation,\n",
    "        'correlation_matrix': correlation_matrix.fillna(0).to_dict(),\n",
    "        'descriptive_statistics': descriptive_stats\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "start_time_stats = now()\n",
    "stats_report = calculate_supermarket_stats(data, target_col='Rating')\n",
    "end_time_stats = now()\n",
    "\n",
    "# Display correlation with target for review\n",
    "if 'error' in stats_report:\n",
    "    print(stats_report['error'])\n",
    "else:\n",
    "    print(\"Correlation with Rating:\\n\",\n",
    "          pd.Series(stats_report['correlation_with_target']).sort_values(ascending=False))\n",
    "    print(\"\\nDescriptive Statistics:\\n\", pd.DataFrame(stats_report['descriptive_statistics'])) \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "stats_ass_uuid_executor = \"274f0808-1b61-4d39-ac33-cdd29e6f2344\"\n",
    "stats_ass_uuid_writer   = \"b4f3e76d-1805-41ea-9428-8fc85c533ede\"\n",
    "\n",
    "stats_comment = \"\"\"\n",
    "Statistical properties and correlations were computed for all numeric attributes of\n",
    "Supermarket Sales dataset. The dataset contains 1,000 complete records with no missing\n",
    "values.\n",
    "\n",
    "Descriptive statistics show that unit prices range is from approximately 10 to 100, with a\n",
    "mean = 55.7. Customers purchase on usually 5 to 6 items per transaction, with\n",
    "quantities which have range from 1 to 10. Sales amounts and cost-related attributes \n",
    "The gross margin percentage is constant across all records (4.76%),\n",
    "which results in zero variance.\n",
    "\n",
    "Customer ratings range from 4 to 10, with mean  approx. 6.97 and standard\n",
    "deviation of 1.72, which indicates moderate variability in customer satisfaction.\n",
    "\n",
    "Pearson correlation analysis with regression target 'Rating' reveals that none of\n",
    "numeric attributes have a strong linear relationship with target variable. All\n",
    "observed correlations are close to zero (|r| < 0.04). Money related attributes such as Sales,\n",
    "cogs, Tax 5%, and gross income show similar correlations with target, which confirms \n",
    "their relationships. The constant gross margin percentage shows no defined\n",
    "correlation.\n",
    "\n",
    "These results indicate that customer satisfaction is not primarily driven by purchase\n",
    "volume or price-related factors only. Instead, categorical attributes, for example, branch,\n",
    "product line, payment method and potential non-linear effects are likely to play a more\n",
    "important role in predicting customer ratings.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stats_activity = [\n",
    "    ':calculate_supermarket_stats rdf:type prov:Activity .',\n",
    "    ':calculate_supermarket_stats sc:isPartOf :data_understanding_phase .',\n",
    "    ':calculate_supermarket_stats rdfs:comment \"Statistical Properties and Correlations\" .',\n",
    "    f':calculate_supermarket_stats rdfs:comment \"\"\"{stats_comment}\"\"\" .',\n",
    "    f':calculate_supermarket_stats prov:startedAtTime \"{start_time_stats}\"^^xsd:dateTime .',\n",
    "    f':calculate_supermarket_stats prov:endedAtTime \"{end_time_stats}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':calculate_supermarket_stats prov:qualifiedAssociation :{stats_ass_uuid_executor} .',\n",
    "    f':{stats_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{stats_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{stats_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':calculate_supermarket_stats prov:qualifiedAssociation :{stats_ass_uuid_writer} .',\n",
    "    f':{stats_ass_uuid_writer} prov:agent :{stats_code_writer} .',\n",
    "    f':{stats_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{stats_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # I/O\n",
    "    ':calculate_supermarket_stats prov:used :data .',\n",
    "    ':stats_report rdf:type prov:Entity .',\n",
    "    f':stats_report rdfs:comment \"\"\"{json.dumps(stats_report, indent=2)}\"\"\" .',\n",
    "    ':stats_report prov:wasGeneratedBy :calculate_supermarket_stats .',\n",
    "]\n",
    "\n",
    "engine.insert(stats_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Skewness:\n",
      " Tax 5%          0.892570\n",
      "gross income    0.892570\n",
      "Sales           0.892570\n",
      "cogs            0.892570\n",
      "Quantity        0.012941\n",
      "Rating          0.009010\n",
      "Unit price      0.007077\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "skew_code_writer = student_a\n",
    "\n",
    "\n",
    "def calculate_distribution_properties(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates skewness for all numerical features in the supermarket dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    numerical_features = [\n",
    "        'Unit price',\n",
    "        'Quantity',\n",
    "        'Tax 5%',\n",
    "        'Sales',\n",
    "        'cogs',\n",
    "        'gross income',\n",
    "        'Rating'\n",
    "    ]\n",
    "\n",
    "    \n",
    "    skewness_series = data[numerical_features].skew()\n",
    "\n",
    "    skewness_report = skewness_series.to_dict()\n",
    "\n",
    "    return skewness_report\n",
    "\n",
    "\n",
    "# Execution and Timing\n",
    "start_time_skew = now()\n",
    "skew_report = calculate_distribution_properties(data)\n",
    "end_time_skew = now()\n",
    "\n",
    "# Display the skewness for quick review\n",
    "print(\"Calculated Skewness:\\n\", pd.Series(skew_report).sort_values(ascending=False))\n",
    "\n",
    "dist_comment = f\"\"\"\n",
    "Skewness was computed for all numeric attributes. Monetary variables\n",
    "(Sales, cogs, Tax 5%, gross income) show positive skewness\n",
    "(approx. 0.89), which indicates right-skewed distributions which were caused by small\n",
    "number of high-value transactions.\n",
    "\n",
    "Unit price, Quantity, and the target variable (Rating) shows very low\n",
    "skewness (close to zero), which indicates approximately symmetric distributions.\n",
    "\n",
    "The observed skewness patterns are typical for retail transaction data and do not indicate data quality issues.\n",
    "\n",
    "Calculated skewness values:\n",
    "{json.dumps(skew_report, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "# Provenance Logging \n",
    "\n",
    "skew_ass_uuid_executor = \"20f6bc55-fce0-4bdc-9e72-bdc18dcbd362\" \n",
    "skew_ass_uuid_writer = \"ec3af855-7b5a-4a76-87bb-149e98c2d3f8\" \n",
    "\n",
    "skew_activity = [\n",
    "    ':calculate_skewness rdf:type prov:Activity .',\n",
    "    ':calculate_skewness sc:isPartOf :data_understanding_phase .',\n",
    "    ':calculate_skewness rdfs:comment \"Distribution and Skewness Check\" .',\n",
    "    f':calculate_skewness rdfs:comment \"\"\"{dist_comment}\"\"\" .',\n",
    "    f':calculate_skewness prov:startedAtTime \"{start_time_skew}\"^^xsd:dateTime .',\n",
    "    f':calculate_skewness prov:endedAtTime \"{end_time_skew}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':calculate_skewness prov:qualifiedAssociation :{skew_ass_uuid_executor} .',\n",
    "    f':{skew_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{skew_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{skew_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':calculate_skewness prov:qualifiedAssociation :{skew_ass_uuid_writer} .',\n",
    "    f':{skew_ass_uuid_writer} prov:agent :{skew_code_writer} .',\n",
    "    f':{skew_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{skew_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # I/O\n",
    "    ':calculate_skewness prov:used :data .',\n",
    "    ':skewness_report rdf:type prov:Entity .',\n",
    "    f':skewness_report rdfs:comment \"\"\"{json.dumps(skew_report, indent=2)}\"\"\" .',\n",
    "    ':skewness_report prov:wasGeneratedBy :calculate_skewness .',\n",
    "]\n",
    "\n",
    "engine.insert(skew_activity, prefixes=prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ebbbe1-01b9-48d9-8651-9df82fbc8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plausibility Summary (Min, Max, Median):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unit price</th>\n",
       "      <td>10.080000</td>\n",
       "      <td>99.960000</td>\n",
       "      <td>55.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tax 5%</th>\n",
       "      <td>0.508500</td>\n",
       "      <td>49.650000</td>\n",
       "      <td>12.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>10.678500</td>\n",
       "      <td>1042.650000</td>\n",
       "      <td>253.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cogs</th>\n",
       "      <td>10.170000</td>\n",
       "      <td>993.000000</td>\n",
       "      <td>241.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gross margin percentage</th>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gross income</th>\n",
       "      <td>0.508500</td>\n",
       "      <td>49.650000</td>\n",
       "      <td>12.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               min          max      median\n",
       "Unit price               10.080000    99.960000   55.230000\n",
       "Quantity                  1.000000    10.000000    5.000000\n",
       "Tax 5%                    0.508500    49.650000   12.088000\n",
       "Sales                    10.678500  1042.650000  253.848000\n",
       "cogs                     10.170000   993.000000  241.760000\n",
       "gross margin percentage   4.761905     4.761905    4.761905\n",
       "gross income              0.508500    49.650000   12.088000\n",
       "Rating                    4.000000    10.000000    7.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plaus_code_writer = student_a\n",
    "\n",
    "def check_value_plausibility(data: pd.DataFrame) -> dict:\n",
    "\n",
    "\n",
    "    numerical_features = [\n",
    "        'Unit price',\n",
    "        'Quantity',\n",
    "        'Tax 5%',\n",
    "        'Sales',\n",
    "        'cogs',\n",
    "        'gross margin percentage',\n",
    "        'gross income',\n",
    "        'Rating'\n",
    "    ]\n",
    "\n",
    "    plausibility_summary = data[numerical_features].agg(['min', 'max', 'median']).T.to_dict()\n",
    "    return plausibility_summary\n",
    "\n",
    "\n",
    "start_time_plaus = now()\n",
    "plausibility_report = check_value_plausibility(data)\n",
    "end_time_plaus = now()\n",
    "\n",
    "\n",
    "print(\"Plausibility Summary (Min, Max, Median):\\n\")\n",
    "display(pd.DataFrame(plausibility_report))\n",
    "\n",
    "plaus_comment = f\"\"\"\n",
    "Plausibility check was done based on minimum, maximum, and median values. It shows that all numeric\n",
    "attributes are located within realistic ranges for supermarket transactions. Unit prices range\n",
    "from about 10 to 100, quantities from 1 to 10 items, and total sales from approximately 11 to\n",
    "1,043. Customer ratings range from 4 to 10.\n",
    "\n",
    "The gross margin percentage is constant across all records, which explains its zero variance.\n",
    "No implausible or erroneous values (e.g., negative prices or quantities) were detected.\n",
    "Plausibility summary:\n",
    "{json.dumps(plausibility_report, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "plaus_ass_uuid_executor = \"c9a7d980-749a-4c45-996e-bc50ad2ce8ca\"\n",
    "plaus_ass_uuid_writer   = \"c9b02d83-a235-4205-a12a-a423bcbf83f5\"\n",
    "\n",
    "plaus_activity = [\n",
    "    ':check_plausibility rdf:type prov:Activity .',\n",
    "    ':check_plausibility sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_plausibility rdfs:comment \"Plausibility of Values Check\" .',\n",
    "    f':check_plausibility rdfs:comment \"\"\"{plaus_comment}\"\"\" .',\n",
    "    f':check_plausibility prov:startedAtTime \"{start_time_plaus}\"^^xsd:dateTime .',\n",
    "    f':check_plausibility prov:endedAtTime \"{end_time_plaus}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':check_plausibility prov:qualifiedAssociation :{plaus_ass_uuid_executor} .',\n",
    "    f':{plaus_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{plaus_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{plaus_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':check_plausibility prov:qualifiedAssociation :{plaus_ass_uuid_writer} .',\n",
    "    f':{plaus_ass_uuid_writer} prov:agent :{plaus_code_writer} .',\n",
    "    f':{plaus_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{plaus_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input/Output\n",
    "    ':check_plausibility prov:used :data .',\n",
    "    ':plausibility_report rdf:type prov:Entity .',\n",
    "    f':plausibility_report rdfs:comment \"\"\"{json.dumps(plausibility_report, indent=2)}\"\"\" .',\n",
    "    ':plausibility_report prov:wasGeneratedBy :check_plausibility .',\n",
    "]\n",
    "\n",
    "engine.insert(plaus_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ef07da2-07b9-44e9-b5f1-4e6563fffbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_code_writer = student_a\n",
    "def visualize_data_properties(data: pd.DataFrame):\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    ax1.hist(data['Rating'], bins=7, edgecolor='black')\n",
    "    ax1.set_title('Distribution of Customer Rating')\n",
    "    ax1.set_xlabel('Rating')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('visual_exploration_rating_distribution.png')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    data.boxplot(\n",
    "        column='Rating',\n",
    "        by='Product line',\n",
    "        ax=ax2,\n",
    "        grid=False,\n",
    "        rot=45\n",
    "    )\n",
    "\n",
    "    ax2.set_title('Customer Rating by Product Line')\n",
    "    ax2.set_xlabel('Product Line')\n",
    "    ax2.set_ylabel('Rating')\n",
    "    plt.suptitle('')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('visual_exploration_rating_product_line.png')\n",
    "    plt.close(fig2)\n",
    "\n",
    "    rating_summary = data['Rating'].describe().to_dict()\n",
    "    product_line_means = data.groupby('Product line')['Rating'].mean().to_dict()\n",
    "\n",
    "    return {\n",
    "        'figures_generated': 2,\n",
    "        'description': 'Histogram of Rating and boxplot of Rating by Product Line.',\n",
    "        'rating_summary': rating_summary,\n",
    "        'mean_rating_by_product_line': product_line_means\n",
    "    }\n",
    "\n",
    "\n",
    "start_time_vis = now()\n",
    "vis_report = visualize_data_properties(data)\n",
    "end_time_vis = now()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vis_ass_uuid_executor = \"bdfb99a2-6c3f-4e8e-b662-9e2cdefe16d7\"\n",
    "vis_ass_uuid_writer   = \"ba832a3a-1e5a-44e1-bbad-34d6d751ca18\"\n",
    "\n",
    "vis_comment = f\"\"\"\n",
    "Visual exploration shows that customer ratings are evenly distributed between\n",
    "4 and 10, with no extreme concentration at single value. This indicates balanced\n",
    "target variable suitable for regression modeling.\n",
    "\n",
    "The boxplot of Rating by Product Line reveals small but noticeable differences in median\n",
    "ratings across product categories, while overall spread of ratings is similar.\n",
    "This suggests that product line may have influence on customer satisfaction, but no\n",
    "single category dominates ratings completely.\n",
    "\n",
    "These observations support inclusion of categorical features, such as product line,\n",
    "in the predictive model.\n",
    "Visual summary:\n",
    "{json.dumps(vis_report, indent=2)}\n",
    "\"\"\"\n",
    "\n",
    "vis_activity = [\n",
    "    ':visualize_data_properties rdf:type prov:Activity .',\n",
    "    ':visualize_data_properties sc:isPartOf :data_understanding_phase .',\n",
    "    ':visualize_data_properties rdfs:comment \"Visual Exploration and Hypothesis Generation\" .',\n",
    "    f':visualize_data_properties rdfs:comment \"\"\"{vis_comment}\"\"\" .',\n",
    "    f':visualize_data_properties prov:startedAtTime \"{start_time_vis}\"^^xsd:dateTime .',\n",
    "    f':visualize_data_properties prov:endedAtTime \"{end_time_vis}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':visualize_data_properties prov:qualifiedAssociation :{vis_ass_uuid_executor} .',\n",
    "    f':{vis_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{vis_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':visualize_data_properties prov:qualifiedAssociation :{vis_ass_uuid_writer} .',\n",
    "    f':{vis_ass_uuid_writer} prov:agent :{vis_code_writer} .',\n",
    "    f':{vis_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input/Output\n",
    "    ':visualize_data_properties prov:used :data .',\n",
    "    ':visual_analysis_report rdf:type prov:Entity .',\n",
    "    f':visual_analysis_report rdfs:label \"2d Visual Exploration\" .', \n",
    "    f':visual_analysis_report rdfs:comment \"\"\"{json.dumps(vis_report, indent=2)}\"\"\" .',\n",
    "    ':visual_analysis_report prov:wasGeneratedBy :visualize_data_properties .',\n",
    "]\n",
    "\n",
    "engine.insert(vis_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbfc5b58-ac9b-43a0-b3ab-d5141fa36048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Comment for 2e ---\n",
    "bias_report_comment = \"\"\"\n",
    "Ethical risks:\n",
    "Ethically sensitive attributes are present in Supermarket Sales dataset,  potentially\n",
    "Gender and Customer type (Member vs. Normal). There is high chance that thhese attributes could potentially introduce\n",
    "bias in the model if model predictions are used to treat customers in different way based on predicted\n",
    "satisfaction.\n",
    "\n",
    "Unbalanced Distributions and Bias Risk:\n",
    "Target variable (Rating) is well\n",
    "distributed across its range (4–10), and no extremes of low or high\n",
    "ratings was observed. Therefore, we assume that there is no bias which arises from target imbalance.\n",
    "\n",
    "However, the usage of predicted ratings to prioritize service, offers, or attention for specific\n",
    "customer groups could lead to unfair treatment. To overcome this risk, sensitive attributes\n",
    "should be handled carefully and used only for analysis, not for decision\n",
    "making.\n",
    "\n",
    "Model evaluation should focus on overall performance (e.g., MAE) and include subgroup\n",
    "analysis (e.g., by customer type or gender) to ensure consistent prediction quality across\n",
    "different customer groups.\n",
    "\"\"\"\n",
    "\n",
    "ass_uuid_executor_2e = \"2ec90a18-1eea-4c77-a88c-0bcf94e8d651\" \n",
    "ass_uuid_writer_2e = \"d80ad7df-17a6-402e-9ad6-5316993b8e6c\"\n",
    "\n",
    "start_time_2e = now()\n",
    "end_time_2e = now()\n",
    "\n",
    "log_bias_activity = [\n",
    "    ':log_bias_evaluation rdf:type prov:Activity .', \n",
    "    ':log_bias_evaluation sc:isPartOf :data_understanding_phase .',\n",
    "    f':log_bias_evaluation rdfs:comment \\' Bias Evaluation Logging\\' .',\n",
    "\n",
    "    # Time of execution\n",
    "    f':log_bias_evaluation prov:startedAtTime \"{start_time_2e}\"^^xsd:dateTime .',\n",
    "    f':log_bias_evaluation prov:endedAtTime \"{end_time_2e}\"^^xsd:dateTime .',\n",
    "\n",
    "    \n",
    "    # Executor Documentation\n",
    "    f':log_bias_evaluation prov:qualifiedAssociation :{ass_uuid_executor_2e} .',\n",
    "    f':{ass_uuid_executor_2e} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_2e} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_2e} prov:hadRole :{code_executor_role} .',\n",
    "    \n",
    "    # Writer Documentation\n",
    "    f':log_bias_evaluation prov:qualifiedAssociation :{ass_uuid_writer_2e} .',\n",
    "    f':{ass_uuid_writer_2e} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_2e} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_2e} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # Entity 2e: Bias Evaluation\n",
    "    f':du_bias_evaluation rdf:type prov:Entity .',\n",
    "    f':du_bias_evaluation prov:wasGeneratedBy :log_bias_evaluation .',\n",
    "    f':du_bias_evaluation rdfs:label \"2e Bias Evaluation\" .',\n",
    "    f':du_bias_evaluation rdfs:comment \"\"\"{bias_report_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_bias_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bf3f4a9-9ca2-47a7-80ec-ce1493166536",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks_expert_comment = \"\"\"\n",
    "Potential Risks and Bias:\n",
    "1. Behavioral Noise: Customer ratings are subjective and can be influenced by not constant\n",
    "   factors such as mood, time pressure, or individual expectations. This can introduce\n",
    "   noise which cannot be fully explained by transaction data alone.\n",
    "\n",
    "2. Omitted Variables: Dataset does not detect important contextual factors such as\n",
    "   staff behavior, waiting time, promotions, or how much the store is crowded, which may significantly\n",
    "   influence customer satisfaction.\n",
    "\n",
    "3. Usage Bias: If predicted ratings are used to prioritize service or offers, this could\n",
    "   lead to unfair treatment for certain customer groups (e.g., Normal vs. Member customers).\n",
    "\n",
    "Questions for an External Expert:\n",
    "1. Rating Process: Under what conditions are ratings collected, and how consistent is the rating behavior\n",
    "   across customers?\n",
    "2. Operational Factors: Are there store-level or operational variables (e.g., staffing,\n",
    "   queue length, time of day effects) that could be added to better explain\n",
    "   customer satisfaction?\n",
    "3. Ethical Use: What should be applied to ensure predicted ratings are used\n",
    "   to improve service quality rather than to discriminate between customer groups?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "ass_uuid_executor_2f = \"04a6cacd-7a5c-4e40-b7d4-a5b600762a8e\"\n",
    "ass_uuid_writer_2f = \"720c7834-f755-4384-9c3f-de1db091c6dd\" \n",
    "\n",
    "# We need to have time of execution which we will add to graph\n",
    "start_time_2f = now()\n",
    "end_time_2f = now()\n",
    "\n",
    "\n",
    "log_risks_activity = [\n",
    "    ':log_risks_expert_questions rdf:type prov:Activity .',\n",
    "    ':log_risks_expert_questions sc:isPartOf :data_understanding_phase .',\n",
    "    f':log_risks_expert_questions rdfs:comment \\'Risks and Expert Questions Logging\\' .',\n",
    "    \n",
    "    f':log_risks_expert_questions prov:startedAtTime \"{start_time_2f}\"^^xsd:dateTime .',\n",
    "    f':log_risks_expert_questions prov:endedAtTime \"{end_time_2f}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor Documentation\n",
    "    f':log_risks_expert_questions prov:qualifiedAssociation :{ass_uuid_executor_2f} .',\n",
    "    f':{ass_uuid_executor_2f} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_2f} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_2f} prov:hadRole :{code_executor_role} .',\n",
    "    \n",
    "    # Writer Documentation\n",
    "    f':log_risks_expert_questions prov:qualifiedAssociation :{ass_uuid_writer_2f} .',\n",
    "    f':{ass_uuid_writer_2f} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_2f} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_2f} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # Entity 2f: Risks and Expert Questions\n",
    "    f':du_risks_and_expert_questions rdf:type prov:Entity .',\n",
    "    f':du_risks_and_expert_questions prov:wasGeneratedBy :log_risks_expert_questions .',\n",
    "    f':du_risks_and_expert_questions rdfs:label \"2f Risks and Expert Questions\" .',\n",
    "    f':du_risks_and_expert_questions rdfs:comment \"\"\"{risks_expert_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_risks_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5469ef1-7f75-450c-ba25-d6acd8b2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_actions_comment = \"\"\"\n",
    "Based on the Data Understanding analysis, following actions are required in\n",
    "Data Preparation phase (Section 3):\n",
    "\n",
    "1. Feature Encoding: Categorical attributes such as Branch, City, Customer\n",
    "   type, Gender, Product line, and Payment must be encoded so they can be usable by regression models.\n",
    "\n",
    "2. Feature Scaling (Recommended): Numerical attributes (e.g., Unit price, Quantity,\n",
    "   Sales, cogs, gross income) should be scaled (e.g., using StandardScaler) to ensure\n",
    "   they could be comparable across features, especially for models sensitive to feature scale.\n",
    "\n",
    "3. Feature Selection / Redundancy Handling: Strongly related monetary attributes\n",
    "   (Sales, cogs, Tax 5%, gross income) should be reviewed to avoid multicollinearity.\n",
    "   One or more redundant variables may be removed or combined.\n",
    "\n",
    "4. Constant Feature Removal: The attribute 'gross margin percentage' is constant and\n",
    "   should be removed because it provides no predictive information.\n",
    "\n",
    "5. Outlier Handling (Optional): Detected outliers represent valid high-value\n",
    "   transactions and do not require removal. Robust transformations may be considered\n",
    "   if required by specific models.\n",
    "\n",
    "6. Extract simple date features: From the 'Date' attribute, extract simple features such as day of week or month.\n",
    "This is done because regression models cannot directly interpret date values. \n",
    "\n",
    "7. Target Variable Preparation: The target variable (Rating) requires no transformation\n",
    "   and can be used directly for regression modeling.\n",
    "\"\"\"\n",
    "\n",
    "# --- Provenance Logging ---\n",
    "\n",
    "ass_uuid_executor_2g = \"d7ad890c-8611-41c3-80f9-b3381fcacfc9\" \n",
    "ass_uuid_writer_2g = \"24d726ce-4f5f-416a-a840-9261b84ea790\"  \n",
    "\n",
    "start_time_2g = now()\n",
    "end_time_2g = now()\n",
    "\n",
    "\n",
    "log_prep_activity = [\n",
    "    ':log_prep_actions rdf:type prov:Activity .', \n",
    "    ':log_prep_actions sc:isPartOf :data_understanding_phase .',\n",
    "    f':log_prep_actions rdfs:comment \\'Required Data Preparation Actions Logging\\' .',\n",
    "    \n",
    "    f':log_prep_actions prov:startedAtTime \"{start_time_2g}\"^^xsd:dateTime .',\n",
    "    f':log_prep_actions prov:endedAtTime \"{end_time_2g}\"^^xsd:dateTime .',\n",
    "\n",
    "\n",
    "    # Executor Documentation\n",
    "    f':log_prep_actions prov:qualifiedAssociation :{ass_uuid_executor_2g} .',\n",
    "    f':{ass_uuid_executor_2g} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_2g} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_2g} prov:hadRole :{code_executor_role} .',\n",
    "    \n",
    "    # Writer Documentation\n",
    "    f':log_prep_actions prov:qualifiedAssociation :{ass_uuid_writer_2g} .',\n",
    "    f':{ass_uuid_writer_2g} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_2g} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_2g} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # Entity 2g: Required Data Preparation Actions\n",
    "    f':du_required_prep_actions rdf:type prov:Entity .',\n",
    "    f':du_required_prep_actions prov:wasGeneratedBy :log_prep_actions .',\n",
    "    f':du_required_prep_actions rdfs:label \"2g Required Data Preparation Actions\" .',\n",
    "    f':du_required_prep_actions rdfs:comment \"\"\"{prep_actions_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_prep_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    cleaned_df = df.copy()\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "start_time_td = now()\n",
    "cleaned_data = handle_outliers(data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "display(cleaned_data.shape)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"704debf6-bbd5-4076-9a43-e0987ffcfa77\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"46869833-1cc5-4c0a-9404-ca7fc8bc1244\"\n",
    "\n",
    "td_comment = \"\"\"\n",
    "Outlier handling was executed based on the Data Understanding decision.\n",
    "The detected outliers correspond to valid high-value supermarket transactions and were\n",
    "therefore retained. No rows were removed in this step.\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \"Data Preparation – Outlier Handling\" .',\n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .',\n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Writer\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "\n",
    "    # Outputs\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e20b8e8-7d7f-4df5-ba38-62704f020c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch</th>\n",
       "      <th>City</th>\n",
       "      <th>Customer type</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Product line</th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Payment</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>74.69</td>\n",
       "      <td>7</td>\n",
       "      <td>548.9715</td>\n",
       "      <td>1/5/2019</td>\n",
       "      <td>1:08:00 PM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>26.1415</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Giza</td>\n",
       "      <td>Naypyitaw</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Electronic accessories</td>\n",
       "      <td>15.28</td>\n",
       "      <td>5</td>\n",
       "      <td>80.2200</td>\n",
       "      <td>3/8/2019</td>\n",
       "      <td>10:29:00 AM</td>\n",
       "      <td>Cash</td>\n",
       "      <td>3.8200</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Home and lifestyle</td>\n",
       "      <td>46.33</td>\n",
       "      <td>7</td>\n",
       "      <td>340.5255</td>\n",
       "      <td>3/3/2019</td>\n",
       "      <td>1:23:00 PM</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>16.2155</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>58.22</td>\n",
       "      <td>8</td>\n",
       "      <td>489.0480</td>\n",
       "      <td>1/27/2019</td>\n",
       "      <td>8:33:00 PM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>23.2880</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Sports and travel</td>\n",
       "      <td>86.31</td>\n",
       "      <td>7</td>\n",
       "      <td>634.3785</td>\n",
       "      <td>2/8/2019</td>\n",
       "      <td>10:37:00 AM</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>30.2085</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch       City Customer type  Gender            Product line  Unit price  \\\n",
       "0   Alex     Yangon        Member  Female       Health and beauty       74.69   \n",
       "1   Giza  Naypyitaw        Normal  Female  Electronic accessories       15.28   \n",
       "2   Alex     Yangon        Normal  Female      Home and lifestyle       46.33   \n",
       "3   Alex     Yangon        Member  Female       Health and beauty       58.22   \n",
       "4   Alex     Yangon        Member  Female       Sports and travel       86.31   \n",
       "\n",
       "   Quantity     Sales       Date         Time      Payment  gross income  \\\n",
       "0         7  548.9715   1/5/2019   1:08:00 PM      Ewallet       26.1415   \n",
       "1         5   80.2200   3/8/2019  10:29:00 AM         Cash        3.8200   \n",
       "2         7  340.5255   3/3/2019   1:23:00 PM  Credit card       16.2155   \n",
       "3         8  489.0480  1/27/2019   8:33:00 PM      Ewallet       23.2880   \n",
       "4         7  634.3785   2/8/2019  10:37:00 AM      Ewallet       30.2085   \n",
       "\n",
       "   Rating  \n",
       "0     9.1  \n",
       "1     9.6  \n",
       "2     7.4  \n",
       "3     8.4  \n",
       "4     5.3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['Branch', 'City', 'Customer type', 'Gender', 'Product line',\n",
       "       'Unit price', 'Quantity', 'Sales', 'Date', 'Time', 'Payment',\n",
       "       'gross income', 'Rating'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_selection_code_writer = student_b\n",
    "\n",
    "def select_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes non-informative and redundant features based on Data Understanding.\n",
    "    \"\"\"\n",
    "    columns_to_drop = [\n",
    "        'Invoice ID',\n",
    "        'gross margin percentage',\n",
    "        'Tax 5%',\n",
    "        'cogs'\n",
    "    ]\n",
    "\n",
    "    filtered_df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "start_time_fs = now()\n",
    "filtered_data = select_features(cleaned_data)\n",
    "end_time_fs = now()\n",
    "\n",
    "display(filtered_data.head())\n",
    "display(filtered_data.columns)\n",
    "\n",
    "\n",
    "\n",
    "fs_ass_uuid_executor = \"df9ac5dc-c122-4dea-a9c5-a29a5f02508a\"\n",
    "\n",
    "feature_selection_executor = [\n",
    "    f':select_features prov:qualifiedAssociation :{fs_ass_uuid_executor} .',\n",
    "    f':{fs_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{fs_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{fs_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(feature_selection_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "fs_ass_uuid_writer = \"55010da8-d41d-4eb3-a585-eca1e621f5fd\"\n",
    "\n",
    "fs_comment = \"\"\"\n",
    "Feature selection was performed based on Data Understanding results.\n",
    "Non-predictive identifiers such ar Invoice ID, constant attributes which refers to \"gross margin percentage\", \n",
    "and redundant monetary features like \"Tax 5%\" and \"cogs\" were removed to reduce dimensionality and similarly corelated variables.\n",
    "\"\"\"\n",
    "\n",
    "feature_selection_activity = [\n",
    "    ':select_features rdf:type prov:Activity .',\n",
    "    ':select_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':select_features rdfs:comment \"Data Preparation – Feature Selection\" .',\n",
    "    f':select_features rdfs:comment \"\"\"{fs_comment}\"\"\" .',\n",
    "    f':select_features prov:startedAtTime \"{start_time_fs}\"^^xsd:dateTime .',\n",
    "    f':select_features prov:endedAtTime \"{end_time_fs}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Writer\n",
    "    f':select_features prov:qualifiedAssociation :{fs_ass_uuid_writer} .',\n",
    "    f':{fs_ass_uuid_writer} prov:agent :{feature_selection_code_writer} .',\n",
    "    f':{fs_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{fs_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input / Output\n",
    "    ':select_features prov:used :cleaned_data .',\n",
    "    ':filtered_data rdf:type prov:Entity .',\n",
    "    ':filtered_data prov:wasGeneratedBy :select_features .',\n",
    "    ':filtered_data prov:wasDerivedFrom :cleaned_data .',\n",
    "]\n",
    "engine.insert(feature_selection_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70c45465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vidak\\AppData\\Local\\Temp\\ipykernel_28360\\139071055.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_new['Time'] = pd.to_datetime(df_new['Time'], errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  day_of_week  hour  minute\n",
       "0      1            5    13       8\n",
       "1      3            4    10      29\n",
       "2      3            6    13      23\n",
       "3      1            6    20      33\n",
       "4      2            4    10      37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datetime_feat_code_writer = student_b\n",
    "\n",
    "\n",
    "def extract_datetime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Parse Date (safe)\n",
    "    df_new['Date'] = pd.to_datetime(df_new['Date'])\n",
    "    df_new['month'] = df_new['Date'].dt.month\n",
    "    df_new['day_of_week'] = df_new['Date'].dt.dayofweek  # 0 = Monday\n",
    "\n",
    "    # Parse Time (AM/PM safe – let pandas infer format)\n",
    "    df_new['Time'] = pd.to_datetime(df_new['Time'], errors='coerce')\n",
    "    df_new['hour'] = df_new['Time'].dt.hour\n",
    "    df_new['minute'] = df_new['Time'].dt.minute\n",
    "\n",
    "    # Drop original columns\n",
    "    df_new = df_new.drop(columns=['Date', 'Time'])\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "start_time_dt = now()\n",
    "datetime_feature_data = extract_datetime_features(filtered_data)\n",
    "end_time_dt = now()\n",
    "\n",
    "display(datetime_feature_data[[ 'month', 'day_of_week', 'hour', 'minute']].head())\n",
    "\n",
    "dt_ass_uuid_executor = \"beac77fb-f817-4e06-8bad-af0eb6170101\"\n",
    "\n",
    "datetime_feature_executor = [\n",
    "    f':extract_datetime_features prov:qualifiedAssociation :{dt_ass_uuid_executor} .',\n",
    "    f':{dt_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dt_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dt_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(datetime_feature_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "dt_ass_uuid_writer = \"695496a2-60ba-46c2-8872-431974d465cf\"\n",
    "\n",
    "dt_comment = \"\"\"\n",
    "Raw Date and Time attributes were transformed into interpretable temporal\n",
    "features (month, day of week, hour and minute) to capture seasonal, weekly, and\n",
    "intra-day effects. The original Date and Time columns were removed to avoid\n",
    "incorrect numerical interpretation.\n",
    "\"\"\"\n",
    "\n",
    "dt_activity = [\n",
    "    ':extract_datetime_features rdf:type prov:Activity .',\n",
    "    ':extract_datetime_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':extract_datetime_features rdfs:comment \"Data Preparation – Date & Time Feature Extraction\" .',\n",
    "    f':extract_datetime_features rdfs:comment \"\"\"{dt_comment}\"\"\" .',\n",
    "    f':extract_datetime_features prov:startedAtTime \"{start_time_dt}\"^^xsd:dateTime .',\n",
    "    f':extract_datetime_features prov:endedAtTime \"{end_time_dt}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Writer\n",
    "    f':extract_datetime_features prov:qualifiedAssociation :{dt_ass_uuid_writer} .',\n",
    "    f':{dt_ass_uuid_writer} prov:agent :{datetime_feat_code_writer} .',\n",
    "    f':{dt_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dt_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input / Output\n",
    "    ':extract_datetime_features prov:used :filtered_data .',\n",
    "    ':datetime_feature_data rdf:type prov:Entity .',\n",
    "    ':datetime_feature_data prov:wasGeneratedBy :extract_datetime_features .',\n",
    "    ':datetime_feature_data prov:wasDerivedFrom :filtered_data .',\n",
    "]\n",
    "engine.insert(dt_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e447e864-ca19-41de-b61a-e2e73863ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Sales</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>Branch_Cairo</th>\n",
       "      <th>...</th>\n",
       "      <th>City_Yangon</th>\n",
       "      <th>Customer type_Normal</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Product line_Fashion accessories</th>\n",
       "      <th>Product line_Food and beverages</th>\n",
       "      <th>Product line_Health and beauty</th>\n",
       "      <th>Product line_Home and lifestyle</th>\n",
       "      <th>Product line_Sports and travel</th>\n",
       "      <th>Payment_Credit card</th>\n",
       "      <th>Payment_Ewallet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>548</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>340</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>489</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>634</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unit price  Quantity  Sales  gross income  Rating  month  day_of_week  \\\n",
       "0          74         7    548            26       9      1            5   \n",
       "1          15         5     80             3       9      3            4   \n",
       "2          46         7    340            16       7      3            6   \n",
       "3          58         8    489            23       8      1            6   \n",
       "4          86         7    634            30       5      2            4   \n",
       "\n",
       "   hour  minute  Branch_Cairo  ...  City_Yangon  Customer type_Normal  \\\n",
       "0    13       8             0  ...            1                     0   \n",
       "1    10      29             0  ...            0                     1   \n",
       "2    13      23             0  ...            1                     1   \n",
       "3    20      33             0  ...            1                     0   \n",
       "4    10      37             0  ...            1                     0   \n",
       "\n",
       "   Gender_Male  Product line_Fashion accessories  \\\n",
       "0            0                                 0   \n",
       "1            0                                 0   \n",
       "2            0                                 0   \n",
       "3            0                                 0   \n",
       "4            0                                 0   \n",
       "\n",
       "   Product line_Food and beverages  Product line_Health and beauty  \\\n",
       "0                                0                               1   \n",
       "1                                0                               0   \n",
       "2                                0                               0   \n",
       "3                                0                               1   \n",
       "4                                0                               0   \n",
       "\n",
       "   Product line_Home and lifestyle  Product line_Sports and travel  \\\n",
       "0                                0                               0   \n",
       "1                                0                               0   \n",
       "2                                1                               0   \n",
       "3                                0                               0   \n",
       "4                                0                               1   \n",
       "\n",
       "   Payment_Credit card  Payment_Ewallet  \n",
       "0                    0                1  \n",
       "1                    0                0  \n",
       "2                    1                0  \n",
       "3                    0                1  \n",
       "4                    0                1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unit price', 'Quantity', 'Sales', 'gross income', 'Rating', 'month',\n",
       "       'day_of_week', 'hour', 'minute', 'Branch_Cairo', 'Branch_Giza',\n",
       "       'City_Naypyitaw', 'City_Yangon', 'Customer type_Normal', 'Gender_Male',\n",
       "       'Product line_Fashion accessories', 'Product line_Food and beverages',\n",
       "       'Product line_Health and beauty', 'Product line_Home and lifestyle',\n",
       "       'Product line_Sports and travel', 'Payment_Credit card',\n",
       "       'Payment_Ewallet'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode_code_writer = student_b\n",
    "\n",
    "def encode_categorical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    categorical_cols = [\n",
    "        'Branch',\n",
    "        'City',\n",
    "        'Customer type',\n",
    "        'Gender',\n",
    "        'Product line',\n",
    "        'Payment'\n",
    "    ]\n",
    "\n",
    "    encoded_df = pd.get_dummies(\n",
    "        df,\n",
    "        columns=categorical_cols,\n",
    "        drop_first=True  \n",
    "    )\n",
    "    encoded_df = encoded_df.astype(int, errors='ignore')\n",
    "    return encoded_df\n",
    "\n",
    "\n",
    "start_time_enc = now()\n",
    "encoded_data = encode_categorical_features(datetime_feature_data)\n",
    "end_time_enc = now()\n",
    "\n",
    "display(encoded_data.head())\n",
    "display(encoded_data.columns)\n",
    "display(encoded_data.shape)\n",
    "\n",
    "\n",
    "enc_ass_uuid_executor = \"fcf64e1a-bb44-4276-8fee-9637d1f7021c\"\n",
    "\n",
    "encode_executor = [\n",
    "    f':encode_categorical_features prov:qualifiedAssociation :{enc_ass_uuid_executor} .',\n",
    "    f':{enc_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{enc_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{enc_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(encode_executor, prefixes=prefixes)\n",
    "\n",
    "enc_ass_uuid_writer = \"3fa20e58-9562-4ed1-97c9-a7a99301a516\"\n",
    "\n",
    "enc_comment = \"\"\"Categorical attributes were transformed using One-Hot Encoding. The reason for it is to avoid introducing artificial ordering or distance assumptions, because this kind of categorical variables does not have ordinal relationship.This tries to ensure correct interpretation by regression models and improves model robustness. One-hot encoding was applied with setting parameter to True, which results in implicit reference categories for each categorical variable. For example, Gender_Female and CustomerType_Member are represented as baseline (value 0) and they are not explicitly stored as separate variables. \"\"\"\n",
    "\n",
    "encode_activity = [\n",
    "    ':encode_categorical_features rdf:type prov:Activity .',\n",
    "    ':encode_categorical_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':encode_categorical_features rdfs:comment \"Data Preparation – One-Hot Encoding\" .',\n",
    "    f':encode_categorical_features rdfs:comment \"\"\"{enc_comment}\"\"\" .',\n",
    "    f':encode_categorical_features prov:startedAtTime \"{start_time_enc}\"^^xsd:dateTime .',\n",
    "    f':encode_categorical_features prov:endedAtTime \"{end_time_enc}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Writer\n",
    "    f':encode_categorical_features prov:qualifiedAssociation :{enc_ass_uuid_writer} .',\n",
    "    f':{enc_ass_uuid_writer} prov:agent :{encode_code_writer} .',\n",
    "    f':{enc_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{enc_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input / Output\n",
    "    ':encode_categorical_features prov:used :filtered_data .',\n",
    "    ':encoded_data rdf:type prov:Entity .',\n",
    "    ':encoded_data prov:wasGeneratedBy :encode_categorical_features .',\n",
    "    ':encoded_data prov:wasDerivedFrom :filtered_data .',\n",
    "]\n",
    "engine.insert(encode_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8e64002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Sales</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>Branch_Cairo</th>\n",
       "      <th>...</th>\n",
       "      <th>City_Yangon</th>\n",
       "      <th>Customer type_Normal</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Product line_Fashion accessories</th>\n",
       "      <th>Product line_Food and beverages</th>\n",
       "      <th>Product line_Health and beauty</th>\n",
       "      <th>Product line_Home and lifestyle</th>\n",
       "      <th>Product line_Sports and travel</th>\n",
       "      <th>Payment_Credit card</th>\n",
       "      <th>Payment_Ewallet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.711694</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.917693</td>\n",
       "      <td>0.951427</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.517427</td>\n",
       "      <td>-0.174540</td>\n",
       "      <td>-0.986614</td>\n",
       "      <td>-1.013096</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.346194</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.071334</td>\n",
       "      <td>0.097287</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.107187</td>\n",
       "      <td>0.852165</td>\n",
       "      <td>0.677620</td>\n",
       "      <td>0.695185</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.165074</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>1.267629</td>\n",
       "      <td>1.293083</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unit price  Quantity     Sales  gross income  Rating  month  day_of_week  \\\n",
       "0    0.711694  0.509930  0.917693      0.951427       9      1            5   \n",
       "1   -1.517427 -0.174540 -0.986614     -1.013096       9      3            4   \n",
       "2   -0.346194  0.509930  0.071334      0.097287       7      3            6   \n",
       "3    0.107187  0.852165  0.677620      0.695185       8      1            6   \n",
       "4    1.165074  0.509930  1.267629      1.293083       5      2            4   \n",
       "\n",
       "   hour  minute  Branch_Cairo  ...  City_Yangon  Customer type_Normal  \\\n",
       "0    13       8             0  ...            1                     0   \n",
       "1    10      29             0  ...            0                     1   \n",
       "2    13      23             0  ...            1                     1   \n",
       "3    20      33             0  ...            1                     0   \n",
       "4    10      37             0  ...            1                     0   \n",
       "\n",
       "   Gender_Male  Product line_Fashion accessories  \\\n",
       "0            0                                 0   \n",
       "1            0                                 0   \n",
       "2            0                                 0   \n",
       "3            0                                 0   \n",
       "4            0                                 0   \n",
       "\n",
       "   Product line_Food and beverages  Product line_Health and beauty  \\\n",
       "0                                0                               1   \n",
       "1                                0                               0   \n",
       "2                                0                               0   \n",
       "3                                0                               1   \n",
       "4                                0                               0   \n",
       "\n",
       "   Product line_Home and lifestyle  Product line_Sports and travel  \\\n",
       "0                                0                               0   \n",
       "1                                0                               0   \n",
       "2                                1                               0   \n",
       "3                                0                               0   \n",
       "4                                0                               1   \n",
       "\n",
       "   Payment_Credit card  Payment_Ewallet  \n",
       "0                    0                1  \n",
       "1                    0                0  \n",
       "2                    1                0  \n",
       "3                    0                1  \n",
       "4                    0                1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Sales</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>Branch_Cairo</th>\n",
       "      <th>...</th>\n",
       "      <th>City_Yangon</th>\n",
       "      <th>Customer type_Normal</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Product line_Fashion accessories</th>\n",
       "      <th>Product line_Food and beverages</th>\n",
       "      <th>Product line_Health and beauty</th>\n",
       "      <th>Product line_Home and lifestyle</th>\n",
       "      <th>Product line_Sports and travel</th>\n",
       "      <th>Payment_Credit card</th>\n",
       "      <th>Payment_Ewallet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.314504e-16</td>\n",
       "      <td>6.394885e-17</td>\n",
       "      <td>2.486900e-17</td>\n",
       "      <td>-3.907985e-17</td>\n",
       "      <td>6.533000</td>\n",
       "      <td>1.993000</td>\n",
       "      <td>3.032000</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>30.09800</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>1.686937</td>\n",
       "      <td>0.835254</td>\n",
       "      <td>1.973543</td>\n",
       "      <td>3.186857</td>\n",
       "      <td>16.88068</td>\n",
       "      <td>0.471167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473946</td>\n",
       "      <td>0.496005</td>\n",
       "      <td>0.495181</td>\n",
       "      <td>0.382704</td>\n",
       "      <td>0.379299</td>\n",
       "      <td>0.359201</td>\n",
       "      <td>0.366789</td>\n",
       "      <td>0.372267</td>\n",
       "      <td>0.463134</td>\n",
       "      <td>0.475606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unit price      Quantity         Sales  gross income    Rating  \\\n",
       "mean  1.314504e-16  6.394885e-17  2.486900e-17 -3.907985e-17  6.533000   \n",
       "std   1.000500e+00  1.000500e+00  1.000500e+00  1.000500e+00  1.686937   \n",
       "\n",
       "         month  day_of_week       hour    minute  Branch_Cairo  ...  \\\n",
       "mean  1.993000     3.032000  14.910000  30.09800      0.332000  ...   \n",
       "std   0.835254     1.973543   3.186857  16.88068      0.471167  ...   \n",
       "\n",
       "      City_Yangon  Customer type_Normal  Gender_Male  \\\n",
       "mean     0.340000              0.435000     0.429000   \n",
       "std      0.473946              0.496005     0.495181   \n",
       "\n",
       "      Product line_Fashion accessories  Product line_Food and beverages  \\\n",
       "mean                          0.178000                         0.174000   \n",
       "std                           0.382704                         0.379299   \n",
       "\n",
       "      Product line_Health and beauty  Product line_Home and lifestyle  \\\n",
       "mean                        0.152000                         0.160000   \n",
       "std                         0.359201                         0.366789   \n",
       "\n",
       "      Product line_Sports and travel  Payment_Credit card  Payment_Ewallet  \n",
       "mean                        0.166000             0.311000         0.345000  \n",
       "std                         0.372267             0.463134         0.475606  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale_code_writer = student_b\n",
    "\n",
    "def scale_numerical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    numerical_cols = [\n",
    "        'Unit price',\n",
    "        'Quantity',\n",
    "        'Sales',\n",
    "        'gross income'\n",
    "    ]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[numerical_cols] = scaler.fit_transform(df_scaled[numerical_cols])\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "start_time_scale = now()\n",
    "scaled_data = scale_numerical_features(encoded_data)\n",
    "end_time_scale = now()\n",
    "\n",
    "display(scaled_data.head())\n",
    "display(scaled_data.describe().loc[['mean', 'std']])\n",
    "\n",
    "\n",
    "\n",
    "scale_ass_uuid_executor = \"ca214b69-1f0f-4b80-9aba-5f13dff8c986\"\n",
    "\n",
    "scale_executor = [\n",
    "    f':scale_numerical_features prov:qualifiedAssociation :{scale_ass_uuid_executor} .',\n",
    "    f':{scale_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{scale_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{scale_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(scale_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "scale_ass_uuid_writer = \"ce7ef93a-e0df-4bbb-9bf1-7fdce96d1726\"\n",
    "\n",
    "scale_comment = \"\"\"\n",
    "Numerical features were standardized using StandardScaler. This is done in order to to ensure comparable\n",
    "feature scales and stable regression behavior. One-hot encoded categorical\n",
    "features and the target variable were excluded from scaling.\n",
    "\"\"\"\n",
    "\n",
    "scale_activity = [\n",
    "    ':scale_numerical_features rdf:type prov:Activity .',\n",
    "    ':scale_numerical_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':scale_numerical_features rdfs:comment \"Data Preparation – Feature Scaling\" .',\n",
    "    f':scale_numerical_features rdfs:comment \"\"\"{scale_comment}\"\"\" .',\n",
    "    f':scale_numerical_features prov:startedAtTime \"{start_time_scale}\"^^xsd:dateTime .',\n",
    "    f':scale_numerical_features prov:endedAtTime \"{end_time_scale}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Writer\n",
    "    f':scale_numerical_features prov:qualifiedAssociation :{scale_ass_uuid_writer} .',\n",
    "    f':{scale_ass_uuid_writer} prov:agent :{scale_code_writer} .',\n",
    "    f':{scale_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{scale_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Input / Output\n",
    "    ':scale_numerical_features prov:used :encoded_data .',\n",
    "    ':scaled_data rdf:type prov:Entity .',\n",
    "    ':scaled_data prov:wasGeneratedBy :scale_numerical_features .',\n",
    "    ':scaled_data prov:wasDerivedFrom :encoded_data .',\n",
    "]\n",
    "engine.insert(scale_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92b82e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_not_applied_comment = \"\"\"\n",
    "Several preprocessing options were evaluated but deliberately not applied.\n",
    "1. Outlier removal was considered based on statistical calculation. However, identified outliers correspond to valid high-value transactions and were retained.\n",
    "2. Label (ordinal) encoding of categorical attributes was rejected to avoid introducing artificial ordering into nominal variables.\n",
    "3. Binning of the target variable (Rating) was not applied, because analysis is formulated as regression task and binning would reduce information granularity.\n",
    "4. The raw Date and Time attributes were not used directly, because regression models cannot meaningfully interpret timestamps without transformation.\n",
    "\"\"\"\n",
    "\n",
    "ass_uuid_executor_3b = \"d295f82b-97ae-42cf-9da3-6053bdf85633\"\n",
    "ass_uuid_writer_3b   = \"156862a3-09a9-4b42-9076-7250fa308a25\"\n",
    "\n",
    "start_time_3b = now()\n",
    "end_time_3b   = now()\n",
    "\n",
    "log_prep_not_applied_activity = [\n",
    "    ':log_prep_not_applied rdf:type prov:Activity .',\n",
    "    ':log_prep_not_applied sc:isPartOf :data_preparation_phase .',\n",
    "    f':log_prep_not_applied rdfs:comment \"3b Preprocessing Steps Considered but Not Applied\" .',\n",
    "\n",
    "    f':log_prep_not_applied prov:startedAtTime \"{start_time_3b}\"^^xsd:dateTime .',\n",
    "    f':log_prep_not_applied prov:endedAtTime \"{end_time_3b}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':log_prep_not_applied prov:qualifiedAssociation :{ass_uuid_executor_3b} .',\n",
    "    f':{ass_uuid_executor_3b} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_3b} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_3b} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':log_prep_not_applied prov:qualifiedAssociation :{ass_uuid_writer_3b} .',\n",
    "    f':{ass_uuid_writer_3b} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_3b} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_3b} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Entity\n",
    "    f':dp_preprocessing_not_applied rdf:type prov:Entity .',\n",
    "    f':dp_preprocessing_not_applied prov:wasGeneratedBy :log_prep_not_applied .',\n",
    "    f':dp_preprocessing_not_applied rdfs:label \"3b Preprocessing Steps Considered but Not Applied\" .',\n",
    "    f':dp_preprocessing_not_applied rdfs:comment \"\"\"{prep_not_applied_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_prep_not_applied_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98523ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_attributes_comment = \"\"\"\n",
    "Derived attributes were introduced to improve interpretability and predictive performance.\n",
    "1. Nominal categorical variables were transformed using one-hot encoding to avoid artificial ordering assumptions.\n",
    "2. Temporal information was extracted from Date and Time attributes by in a way that month, day of week, hour and minute features were derived to capture seasonal, weekly, and intra-day effects.\n",
    "More complex derived attributes were considered but not applied to avoid not necessary feature expansion.\n",
    "\"\"\"\n",
    "\n",
    "ass_uuid_executor_3c = \"3af6613a-d2ce-452a-ad54-90875e3b8772\"\n",
    "ass_uuid_writer_3c   = \"97748357-4ebe-4a43-b899-9d88dec37191\"\n",
    "\n",
    "start_time_3c = now()\n",
    "end_time_3c   = now()\n",
    "\n",
    "log_derived_attributes_activity = [\n",
    "    ':log_derived_attributes rdf:type prov:Activity .',\n",
    "    ':log_derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    f':log_derived_attributes rdfs:comment \"3c Derived Attributes\" .',\n",
    "\n",
    "    f':log_derived_attributes prov:startedAtTime \"{start_time_3c}\"^^xsd:dateTime .',\n",
    "    f':log_derived_attributes prov:endedAtTime \"{end_time_3c}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':log_derived_attributes prov:qualifiedAssociation :{ass_uuid_executor_3c} .',\n",
    "    f':{ass_uuid_executor_3c} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_3c} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_3c} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':log_derived_attributes prov:qualifiedAssociation :{ass_uuid_writer_3c} .',\n",
    "    f':{ass_uuid_writer_3c} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_3c} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_3c} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Entity\n",
    "    f':dp_derived_attributes rdf:type prov:Entity .',\n",
    "    f':dp_derived_attributes prov:wasGeneratedBy :log_derived_attributes .',\n",
    "    f':dp_derived_attributes rdfs:label \"3c Derived Attributes\" .',\n",
    "    f':dp_derived_attributes rdfs:comment \"\"\"{derived_attributes_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_derived_attributes_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7988f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_comment = \"\"\"\n",
    "Additional external data sources could potentially improve the prediction of customer ratings.\n",
    "Examples include promotional calendars, public holidays, local events,\n",
    "or regional economic indicators. This type of data could help to explain contextual and temporal variations\n",
    "in customer satisfaction. These data sources were not integrated because of limited availability.\n",
    "\"\"\"\n",
    "\n",
    "ass_uuid_executor_3d = \"ed069340-5a8d-4539-b60b-cb0872d678d6\"\n",
    "ass_uuid_writer_3d   = \"cde95156-a7ea-44f2-80be-7b5eddfcac48\"\n",
    "\n",
    "start_time_3d = now()\n",
    "end_time_3d   = now()\n",
    "\n",
    "log_external_data_activity = [\n",
    "    ':log_external_data rdf:type prov:Activity .',\n",
    "    ':log_external_data sc:isPartOf :data_preparation_phase .',\n",
    "    f':log_external_data rdfs:comment \"3d External Data Sources and Attributes\" .',\n",
    "\n",
    "    f':log_external_data prov:startedAtTime \"{start_time_3d}\"^^xsd:dateTime .',\n",
    "    f':log_external_data prov:endedAtTime \"{end_time_3d}\"^^xsd:dateTime .',\n",
    "\n",
    "    # Executor\n",
    "    f':log_external_data prov:qualifiedAssociation :{ass_uuid_executor_3d} .',\n",
    "    f':{ass_uuid_executor_3d} prov:agent :{executed_by} .',\n",
    "    f':{ass_uuid_executor_3d} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_executor_3d} prov:hadRole :{code_executor_role} .',\n",
    "\n",
    "    # Writer\n",
    "    f':log_external_data prov:qualifiedAssociation :{ass_uuid_writer_3d} .',\n",
    "    f':{ass_uuid_writer_3d} prov:agent :{student_a} .',\n",
    "    f':{ass_uuid_writer_3d} rdf:type prov:Association .',\n",
    "    f':{ass_uuid_writer_3d} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Entity\n",
    "    f':dp_external_data_sources rdf:type prov:Entity .',\n",
    "    f':dp_external_data_sources prov:wasGeneratedBy :log_external_data .',\n",
    "    f':dp_external_data_sources rdfs:label \"3d External Data Sources and Attributes\" .',\n",
    "    f':dp_external_data_sources rdfs:comment \"\"\"{external_data_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(log_external_data_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "prepared_data = scaled_data.copy()\n",
    "\n",
    "prepared_data_comment = \"\"\" \n",
    "This dataset represents final output of the Data Preparation phase.\n",
    "It includes selected numerical features, one-hot encoded categorical variables,\n",
    "derived temporal attributes (month, day of week, hour, minute), and standardized numerical\n",
    "values. This dataset is fully numeric, reproducible, and ready to be used as input\n",
    "for regression modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prepared_data_triples = [\n",
    "    # Entity type\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "\n",
    "    # Provenance\n",
    "    ':prepared_data prov:wasDerivedFrom :scaled_data .',\n",
    "\n",
    "    # Dataset description\n",
    "    ':prepared_data rdfs:label \"Prepared Dataset for Modeling\" .',\n",
    "    f':prepared_data rdfs:comment \"\"\"{prepared_data_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction done.\n"
     ]
    }
   ],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "NOW_TS = datetime.now(timezone.utc)\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query,yn_timestamp_query=False)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?label ?comment WHERE {{\n",
    "  VALUES ?e {{\n",
    "    :bu_data_source_and_scenario\n",
    "    :bu_business_objectives\n",
    "    :bu_business_success_criteria\n",
    "    :bu_data_mining_goals\n",
    "    :bu_data_mining_success_criteria\n",
    "    :bu_ai_risk_aspects\n",
    "  }}\n",
    "  ?e rdfs:label ?label ;\n",
    "     rdfs:comment ?comment .\n",
    "}}\n",
    "ORDER BY ?label\n",
    "\"\"\"\n",
    "\n",
    "res_bu = engine.query(bu_query, timestamp=NOW_TS)\n",
    "\n",
    "bu_sections = {}\n",
    "\n",
    "if not res_bu.empty:\n",
    "    for _, row in res_bu.iterrows():  # type: ignore\n",
    "        label = clean_rdf(row[\"label\"])\n",
    "        comment = latex_escape(clean_rdf(row[\"comment\"]))\n",
    "        bu_sections[label] = comment\n",
    "\n",
    "        \n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query,timestamp=NOW_TS)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query,timestamp=NOW_TS)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "\n",
    "\n",
    "# section 2.1 Data understanding - Outlier Detection\n",
    "\n",
    "outlier_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :check_outliers rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "res_outlier = engine.query(outlier_comment_query, timestamp=NOW_TS)\n",
    "\n",
    "outlier_title = \"\"\n",
    "outlier_description = \"\"\n",
    "\n",
    "if not res_outlier.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_outlier[\"comment\"]]\n",
    "\n",
    "    # Heuristic: shortest = title, longest = explanation\n",
    "    outlier_title = min(comments, key=len)\n",
    "    outlier_description = max(comments, key=len)\n",
    "\n",
    "\n",
    "outlier_title_latex = latex_escape(outlier_title)\n",
    "outlier_description_latex = latex_escape(outlier_description)\n",
    "\n",
    "outlier_section_latex = f\"\"\"\n",
    "\\\\subsection{{{outlier_title_latex}}}\n",
    "\n",
    "{outlier_description_latex}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# section 2.2 Data understanding - outlier decission\n",
    "\n",
    "inspect_outlier_comments_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :inspect_outlier_report rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_ior = engine.query(inspect_outlier_comments_query, timestamp=NOW_TS)\n",
    "\n",
    "ior_title = \"\"\n",
    "ior_description = \"\"\n",
    "\n",
    "if not res_ior.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_ior[\"comment\"]]\n",
    "\n",
    "    # Shortest = title, longest = explanation\n",
    "    ior_title = min(comments, key=len)\n",
    "    ior_description = max(comments, key=len)\n",
    "\n",
    "ior_title_latex = latex_escape(ior_title)\n",
    "ior_description_latex = latex_escape(ior_description)\n",
    "\n",
    "ior_section_latex = f\"\"\"\n",
    "\\\\subsection{{{ior_title_latex}}}\n",
    "{ior_description_latex}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "outlier_decision_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :outlier_decision rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_decision = engine.query(outlier_decision_query, timestamp=NOW_TS)\n",
    "\n",
    "outlier_decision_text = latex_escape(\n",
    "    clean_rdf(res_decision.iloc[0][\"comment\"]) if not res_decision.empty else \"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# section 2.3 - Statistical Properties and Correlations\n",
    "stats_report_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :stats_report rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_stats = engine.query(stats_report_query, timestamp=NOW_TS)\n",
    "\n",
    "stats_report = {}\n",
    "if not res_stats.empty:  # type: ignore\n",
    "    stats_report = json.loads(clean_rdf(res_stats.iloc[0][\"comment\"]))\n",
    "\n",
    "stats_report[\"correlation_with_target\"]\n",
    "stats_report[\"descriptive_statistics\"]\n",
    "\n",
    "\n",
    "corr = stats_report[\"correlation_with_target\"]\n",
    "\n",
    "corr_rows = []\n",
    "for var, val in corr.items():\n",
    "    val_str = \"NaN\" if val is None else f\"{val:.6f}\"\n",
    "    corr_rows.append(\n",
    "        rf\"{latex_escape(var)} & {val_str} \\\\\"\n",
    "    )\n",
    "\n",
    "\n",
    "corr_table_latex = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\begin{tabular}{lr}\n",
    "\\hline\n",
    "\\textbf{Variable} & \\textbf{Correlation} \\\\\n",
    "\\hline\n",
    "\"\"\" + \"\\n\".join(corr_rows) + r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Correlation of numerical attributes with Rating}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "stats_report[\"descriptive_statistics\"][\"mean\"][\"Unit price\"]\n",
    "\n",
    "desc = stats_report[\"descriptive_statistics\"]\n",
    "\n",
    "table2_rows = []\n",
    "for var in desc[\"mean\"].keys():\n",
    "    row = rf\"{latex_escape(var)} & \" \\\n",
    "          rf\"{desc['count'][var]:.1f} & \" \\\n",
    "          rf\"{desc['mean'][var]:.6f} & \" \\\n",
    "          rf\"{desc['std'][var]:.6f} & \" \\\n",
    "          rf\"{desc['50%'][var]:.6f} \\\\\"\n",
    "    table2_rows.append(row)\n",
    "\n",
    "desc_table_latex_1 = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\scriptsize\n",
    "\\begin{tabular}{lrrrr}\n",
    "\\hline\n",
    "\\textbf{Variable} & \\textbf{count} & \\textbf{mean} & \\textbf{std} & \\textbf{median} \\\\\n",
    "\\hline\n",
    "\"\"\" + \"\\n\".join(table2_rows) + r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Count, mean, standard deviation, and median of numerical attributes}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "table3_rows = []\n",
    "for var in desc[\"min\"].keys():\n",
    "    row = rf\"{latex_escape(var)} & \" \\\n",
    "          rf\"{desc['min'][var]:.6f} & \" \\\n",
    "          rf\"{desc['25%'][var]:.6f} & \" \\\n",
    "          rf\"{desc['75%'][var]:.6f} & \" \\\n",
    "          rf\"{desc['max'][var]:.6f} \\\\\"\n",
    "    table3_rows.append(row)\n",
    "\n",
    "desc_table_latex_2 = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\scriptsize\n",
    "\\begin{tabular}{lrrrr}\n",
    "\\hline\n",
    "\\textbf{Variable} & \\textbf{min} & \\textbf{25\\%} & \\textbf{75\\%} & \\textbf{max} \\\\\n",
    "\\hline\n",
    "\"\"\" + \"\\n\".join(table3_rows) + r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Minimum, quartiles, and maximum of numerical attributes}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# not tested queries because of server issue\n",
    "\n",
    "stats_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :calculate_supermarket_stats rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "res_stats_comment = engine.query(stats_comment_query, timestamp=NOW_TS)\n",
    "\n",
    "stats_title = \"\"\n",
    "stats_text  = \"\"\n",
    "\n",
    "if not res_stats_comment.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_stats_comment[\"comment\"]]\n",
    "    stats_title = min(comments, key=len)\n",
    "    stats_text  = max(comments, key=len)\n",
    "\n",
    "stats_title_latex = latex_escape(stats_title)\n",
    "stats_text_latex  = latex_escape(stats_text)\n",
    "\n",
    "stats_section_latex = f\"\"\"\n",
    "\\\\subsection{{{stats_title_latex}}}\n",
    "\n",
    "{stats_text_latex}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2.4 skeweness\n",
    "\n",
    "skew_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :skewness_report rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_skew = engine.query(skew_query, timestamp=NOW_TS)\n",
    "\n",
    "skew_report = {}\n",
    "\n",
    "if not res_skew.empty:  # type: ignore\n",
    "    skew_report = json.loads(\n",
    "        clean_rdf(res_skew.iloc[0][\"comment\"])\n",
    "    )\n",
    "\n",
    "skew_rows = []\n",
    "\n",
    "# Sort by absolute skewness (descending)\n",
    "for var, val in sorted(skew_report.items(), key=lambda x: -abs(x[1])):\n",
    "    skew_rows.append(\n",
    "        rf\"{latex_escape(var)} & {val:.6f} \\\\\"\n",
    "    )\n",
    "\n",
    "skew_table_rows = \"\\n\".join(skew_rows)\n",
    "\n",
    "\n",
    "skew_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :calculate_skewness rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "res_skew_comment = engine.query(skew_comment_query, timestamp=NOW_TS)\n",
    "\n",
    "skew_title = \"\"\n",
    "skew_text  = \"\"\n",
    "\n",
    "if not res_skew_comment.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_skew_comment[\"comment\"]]\n",
    "    skew_title = min(comments, key=len)\n",
    "    skew_text  = max(comments, key=len)\n",
    "\n",
    "skew_title_latex = latex_escape(skew_title)\n",
    "skew_text_latex  = latex_escape(skew_text)\n",
    "\n",
    "skew_section_latex = f\"\"\"\n",
    "\\\\subsection{{{skew_title_latex}}}\n",
    "{skew_text_latex}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2.5 plausabilioty\n",
    "\n",
    "plaus_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :plausibility_report rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_plaus = engine.query(plaus_query, timestamp=NOW_TS)\n",
    "\n",
    "plausibility_report = {}\n",
    "if not res_plaus.empty:  # type: ignore\n",
    "    plausibility_report = json.loads(\n",
    "        clean_rdf(res_plaus.iloc[0][\"comment\"])\n",
    "    )\n",
    "\n",
    "plaus_rows = []\n",
    "\n",
    "for var in plausibility_report[\"min\"]:\n",
    "    plaus_rows.append(\n",
    "        rf\"{latex_escape(var)} & \"\n",
    "        rf\"{plausibility_report['min'][var]:.6f} & \"\n",
    "        rf\"{plausibility_report['max'][var]:.6f} & \"\n",
    "        rf\"{plausibility_report['median'][var]:.6f} \\\\\"\n",
    "    )\n",
    "\n",
    "\n",
    "plaus_table_latex = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\begin{tabular}{lrrr}\n",
    "\\hline\n",
    "\\textbf{Variable} & \\textbf{min} & \\textbf{max} & \\textbf{median} \\\\\n",
    "\\hline\n",
    "\"\"\" + \"\\n\".join(plaus_rows) + r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Minimum, maximum, and median values of numerical attributes}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "plaus_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :check_plausibility rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_plaus_comment = engine.query(plaus_comment_query, timestamp=NOW_TS)\n",
    "\n",
    "plaus_title = \"\"\n",
    "plaus_text  = \"\"\n",
    "\n",
    "if not res_plaus_comment.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_plaus_comment[\"comment\"]]\n",
    "    plaus_title = min(comments, key=len)\n",
    "    plaus_text  = max(comments, key=len)\n",
    "\n",
    "plaus_title_latex = latex_escape(plaus_title)\n",
    "plaus_text_latex  = latex_escape(plaus_text)\n",
    "\n",
    "plaus_section_latex = f\"\"\"\n",
    "\\\\subsection{{{plaus_title_latex}}}\n",
    "{plaus_text_latex}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2.5 visualisations\n",
    "vis_comments_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :visualize_data_properties rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_vis = engine.query(vis_comments_query, timestamp=NOW_TS)\n",
    "\n",
    "vis_title = \"\"\n",
    "vis_description = \"\"\n",
    "\n",
    "if not res_vis.empty:  # type: ignore\n",
    "    comments = [clean_rdf(c) for c in res_vis[\"comment\"]]\n",
    "\n",
    "    # Convention: short = title, long = explanation\n",
    "    vis_title = min(comments, key=len)\n",
    "    vis_description = max(comments, key=len)\n",
    "\n",
    "vis_title_latex = latex_escape(vis_title)\n",
    "vis_description_latex = latex_escape(vis_description)\n",
    "\n",
    "vis_section_latex = f\"\"\"\n",
    "\\\\subsection{{{vis_title_latex}}}\n",
    "{vis_description_latex}\n",
    "\"\"\"\n",
    "\n",
    "# 2.6 Bias riskts\n",
    "bias_title_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :log_bias_evaluation rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_bias_title = engine.query(bias_title_query, timestamp=NOW_TS)\n",
    "\n",
    "bias_title = latex_escape(\n",
    "    clean_rdf(res_bias_title.iloc[0][\"comment\"]) if not res_bias_title.empty else \"\"\n",
    ")\n",
    "\n",
    "bias_text_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?label ?comment WHERE {{\n",
    "  :du_bias_evaluation rdfs:label ?label .\n",
    "  :du_bias_evaluation rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_bias = engine.query(bias_text_query, timestamp=NOW_TS)\n",
    "\n",
    "bias_label = \"\"\n",
    "bias_text = \"\"\n",
    "\n",
    "if not res_bias.empty:  # type: ignore\n",
    "    bias_label = latex_escape(clean_rdf(res_bias.iloc[0][\"label\"]))\n",
    "    bias_text  = latex_escape(clean_rdf(res_bias.iloc[0][\"comment\"]))\n",
    "\n",
    "bias_section_latex = f\"\"\"\n",
    "\\\\subsection{{{bias_title}}}\n",
    "{bias_text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 2.7 Risks and experts\n",
    "risks_title_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :log_risks_expert_questions rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_risks_title = engine.query(risks_title_query, timestamp=NOW_TS)\n",
    "\n",
    "risks_title = latex_escape(\n",
    "    clean_rdf(res_risks_title.iloc[0][\"comment\"]) if not res_risks_title.empty else \"\"\n",
    ")\n",
    "\n",
    "risks_text_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?label ?comment WHERE {{\n",
    "  :du_risks_and_expert_questions rdfs:label ?label .\n",
    "  :du_risks_and_expert_questions rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_risks = engine.query(risks_text_query, timestamp=NOW_TS)\n",
    "\n",
    "risks_label = \"\"\n",
    "risks_text_raw = \"\"\n",
    "\n",
    "if not res_risks.empty:  # type: ignore\n",
    "    risks_label = latex_escape(clean_rdf(res_risks.iloc[0][\"label\"]))\n",
    "    risks_text_raw = clean_rdf(res_risks.iloc[0][\"comment\"])\n",
    "\n",
    "\n",
    "risks_section_latex = f\"\"\"\n",
    "\\\\subsection{{{risks_title}}}\n",
    "{latex_escape(risks_text_raw)}\n",
    "\"\"\"\n",
    "\n",
    "#2.8 Data Preparation - Prep Actions\n",
    "prep_actions_title_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :log_prep_actions rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_prep_title = engine.query(prep_actions_title_query, timestamp=NOW_TS)\n",
    "\n",
    "prep_actions_title = latex_escape(\n",
    "    clean_rdf(res_prep_title.iloc[0][\"comment\"]) if not res_prep_title.empty else \"\"\n",
    ")\n",
    "\n",
    "prep_actions_text_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?label ?comment WHERE {{\n",
    "  :du_required_prep_actions rdfs:label ?label .\n",
    "  :du_required_prep_actions rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_prep = engine.query(prep_actions_text_query, timestamp=NOW_TS)\n",
    "\n",
    "prep_actions_label = \"\"\n",
    "prep_actions_text_raw = \"\"\n",
    "\n",
    "if not res_prep.empty:  # type: ignore\n",
    "    prep_actions_label = latex_escape(clean_rdf(res_prep.iloc[0][\"label\"]))\n",
    "    prep_actions_text_raw = latex_escape(clean_rdf(res_prep.iloc[0][\"comment\"]))\n",
    "\n",
    "\n",
    "prep_actions_section_latex = f\"\"\"\n",
    "\\\\subsection{{{prep_actions_title}}}\n",
    "{prep_actions_text_raw}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### Data Preparation examples\n",
    "dp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?act ?start ?end ?comment WHERE {{\n",
    "  ?act a prov:Activity ;\n",
    "       sc:isPartOf :data_preparation_phase .\n",
    "  OPTIONAL {{ ?act prov:startedAtTime ?start . }}\n",
    "  OPTIONAL {{ ?act prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{ ?act rdfs:comment ?comment . }}\n",
    "}}\n",
    "ORDER BY ?start\n",
    "\"\"\"\n",
    "res_dp = engine.query(dp_query, timestamp=NOW_TS)\n",
    "\n",
    "\n",
    "best_comment = {}  \n",
    "if not res_dp.empty:\n",
    "    for _, row in res_dp.iterrows():  \n",
    "        act = clean_rdf(row.get(\"act\", \"\"))\n",
    "        start = clean_rdf(row.get(\"start\", \"\"))\n",
    "        c = clean_rdf(row.get(\"comment\", \"\"))\n",
    "        if not c:\n",
    "            continue\n",
    "        \n",
    "        if (act not in best_comment) or (len(c) > len(best_comment[act][1])):\n",
    "            best_comment[act] = (start, c)\n",
    "\n",
    "\n",
    "items = sorted(best_comment.items(), key=lambda kv: kv[1][0] or \"\")\n",
    "\n",
    "\n",
    "dp_steps_latex = \"\\n\".join(\n",
    "    rf\"\\item {latex_escape(c)}\" for _, (_, c) in items\n",
    ")\n",
    "\n",
    "prep_3bcd_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?entity ?label (SAMPLE(?c) AS ?comment)\n",
    "WHERE {{\n",
    "  VALUES ?entity {{\n",
    "    :dp_preprocessing_not_applied\n",
    "    :dp_derived_attributes\n",
    "    :dp_external_data_sources\n",
    "  }}\n",
    "  ?entity rdfs:label ?label .\n",
    "  OPTIONAL {{ ?entity rdfs:comment ?c . }}\n",
    "}}\n",
    "GROUP BY ?entity ?label\n",
    "ORDER BY ?label\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "res_prep_3bcd = engine.query(prep_3bcd_query, timestamp=NOW_TS)\n",
    "\n",
    "prep_3bcd_latex = \"\"\n",
    "\n",
    "if not res_prep_3bcd.empty:  # type: ignore\n",
    "    for _, row in res_prep_3bcd.iterrows():  # type: ignore\n",
    "        title = latex_escape(clean_rdf(row[\"label\"]))\n",
    "        text  = latex_escape(clean_rdf(row.get(\"comment\", \"\")))\n",
    "\n",
    "        prep_3bcd_latex += f\"\"\"\n",
    "\\\\subsubsection{{{title}}}\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prepared_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?label ?comment WHERE {{\n",
    "  :prepared_data rdfs:label ?label .\n",
    "  OPTIONAL {{ :prepared_data rdfs:comment ?comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_prepared = engine.query(prepared_query, timestamp=NOW_TS)\n",
    "row_prep = res_prepared.iloc[0] if not res_prepared.empty else {}  # type: ignore\n",
    "prep_label = latex_escape(clean_rdf(row_prep.get(\"label\", \"\")))\n",
    "prep_comment = latex_escape(clean_rdf(row_prep.get(\"comment\", \"\")))\n",
    "\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query,timestamp=NOW_TS)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query,timestamp=NOW_TS)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{https://doi.org/10.5281/zenodo.17955246}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_sections[\"1a Data Source and Scenario\"] }\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{ bu_sections[\"1b Business Objectives\"] }\n",
    "\n",
    "\\subsection{{Business Success Criteria}}\n",
    "{ bu_sections[\"1c Business Success Criteria\"] }\n",
    "\n",
    "\\subsection{{Data Mining Goals}}\n",
    "{ bu_sections[\"1d Data Mining Goals\"] }\n",
    "\n",
    "\\subsection{{Data Mining Success Criteria}}\n",
    "{bu_sections[\"1e Data Mining Success Criteria\"] }\n",
    "\n",
    "\\subsection{{AI Risk Aspects}}\n",
    "{ bu_sections[\"1f AI risk aspects\"] }\n",
    "\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\n",
    "{outlier_section_latex}\n",
    "\n",
    "{ior_section_latex}\n",
    "\n",
    "\n",
    "\\textbf{{Outlier Decision:}} {outlier_decision_text}\n",
    "\n",
    "{stats_section_latex}\n",
    "\n",
    "{corr_table_latex}\n",
    "\n",
    "{desc_table_latex_1}\n",
    "\n",
    "{desc_table_latex_2}\n",
    "\n",
    "\n",
    "{skew_section_latex}\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "\\centering\n",
    "\\begin{{tabular}}{{lr}}\n",
    "\\hline\n",
    "\\textbf{{Variable}} & \\textbf{{Skewness}} \\\\\n",
    "\\hline\n",
    "{skew_table_rows}\n",
    "\\hline\n",
    "\\end{{tabular}}\n",
    "\\caption{{Calculated skewness values of numerical attributes}}\n",
    "\\label{{tab:skewness}}\n",
    "\\end{{table}}\n",
    "\n",
    "\n",
    "{plaus_section_latex}\n",
    "\n",
    "{plaus_table_latex}\n",
    "\n",
    "{vis_section_latex}\n",
    "\n",
    "\\begin{{figure}}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.5\\textwidth]{{visual_exploration_rating_distribution.png}}\n",
    "    \\caption{{Customer Rating by Product Line}}\n",
    "    \n",
    "\\end{{figure}}\n",
    "\\begin{{figure}}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.5\\textwidth]{{visual_exploration_rating_product_line.png}}\n",
    "    \\caption{{Boxplot of Rating by Product Line}}\n",
    "    \n",
    "\\end{{figure}}\n",
    "\n",
    "\n",
    "{bias_section_latex}\n",
    "\n",
    "\n",
    "{risks_section_latex}\n",
    "\n",
    "\n",
    "{prep_actions_section_latex}\n",
    "\n",
    "\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "\n",
    "The Data Preparation phase transforms the raw supermarket transaction data into a structured\n",
    "and fully numeric dataset suitable for regression modeling. All preprocessing steps described\n",
    "in this section are directly derived from the provenance graph by querying activities that are\n",
    "part of the Data Preparation phase (\\texttt{{sc:isPartOf :data\\_preparation\\_phase}}).\n",
    "\n",
    "\\paragraph{{Documented preprocessing steps.}}\n",
    "The following preprocessing activities were executed and logged in the provenance graph as\n",
    "individual \\texttt{{prov:Activity}} instances, together with their associated comments and\n",
    "execution timestamps:\n",
    "\n",
    "\\begin{{itemize}}\n",
    "{dp_steps_latex}\n",
    "\\end{{itemize}}\n",
    "\n",
    "{prep_3bcd_latex}\n",
    "\n",
    "\n",
    "\n",
    "Each activity explicitly records its input and output datasets using provenance relations\n",
    "such as \\texttt{{prov:used}}, \\texttt{{prov:wasGeneratedBy}}, and \\texttt{{prov:wasDerivedFrom}},\n",
    "ensuring full traceability of all data transformations.\n",
    "\n",
    "\\paragraph{{Final prepared dataset.}}\n",
    "The outcome of the Data Preparation phase is the dataset\n",
    "\\texttt{{:prepared\\_data}} (label: \\emph{{{prep_label}}}), which is documented in the provenance\n",
    "graph as a \\texttt{{prov:Entity}}. The dataset description retrieved from the graph is as follows:\n",
    "\n",
    "\\begin{{quote}}\n",
    "{prep_comment}\n",
    "\\end{{quote}}\n",
    "\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to: data\\report\\experiment_report.tex\n"
     ]
    }
   ],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
