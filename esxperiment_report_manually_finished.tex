\documentclass[sigconf]{acmart}

\AtBeginDocument{ \providecommand\BibTeX{ Bib\TeX } }
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[BI 2025]{Business Intelligence}{-}{-}

\begin{document}

\title{BI2025 Experiment Report - Group 008}
%% ---Authors: Dynamically added ---


\begin{abstract}
  This report documents the machine learning experiment for Group 008, following the CRISP-DM process model.
\end{abstract}

\ccsdesc[500]{Computing methodologies~Machine learning}
\keywords{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}

\maketitle

%% --- 1. Business Understanding ---
\section{Business Understanding}

\subsection{Data Source and Scenario}
The dataset is the Supermarket Sales dataset. It contains transaction-level data including
branch, customer type, gender, product line, payment method, unit price, quantity, tax, total sales, date, time,
and a customer satisfaction rating (4 to 10). Scenario: The supermarket chain wants to monitor customer satisfaction
and predict the rating a customer is likely to give. The business particularly wants to detect low predicted ratings
early to improve service quality and customer experience.

\subsection{Business Objectives}
The primary business objectives are:
1. Understand which factors affects customer satisfaction.
2. Predict customer ratings based on available information.
3. Detect scenarios likely to produce low ratings and support decisions to improve service.

%% --- 2. Data Understanding ---
\section{Data Understanding}
\textbf{Dataset Description:} Transaction-level supermarket sales data including customer attributes, product information, sales amounts, timestamps, and customer ratings.

The following features were identified in the dataset:

\begin{table}[h]
  \caption{Raw Data Features}
  \label{tab:features}
  \begin{tabular}{lp{0.2\linewidth}p{0.4\linewidth}}
    \toprule
    \textbf{Feature Name} & \textbf{Data Type} & \textbf{Description} \\
    \midrule
    Branch & string> & Supermarket branch where the transaction occurred. \\
    City & string> & City where the branch is located. \\
    Customer type & string> & Customer classification: Member or Normal. \\
    Date & date> & Date of transaction. \\
    Gender & string> & Gender of the customer. \\
    Invoice ID & string> & Unique identifier for each transaction. \\
    Payment & string> & Payment method used in the transaction. \\
    Product line & string> & Category of product purchased. \\
    Quantity & integer> & Number of items purchased. \\
    Rating & float> & Customer satisfaction rating on a scale from 4 to 10. \\
    Sales & float> & Total amount paid including tax. \\
    Tax 5\% & float> & 5 percent tax applied to the purchase. \\
    Time & string> & Time of transaction. \\
    Unit price & float> & Price per unit of product. \\
    cogs & float> & Cost of goods sold (pre-tax). \\
    gross income & float> & Gross income from the transaction. \\
    gross margin percentage & float> & Gross margin percentage (constant in dataset). \\
    \bottomrule
  \end{tabular}
\end{table}

%% --- 3. Data Preparation ---
\section{Data Preparation}
\subsection{Data Cleaning}

The Data Preparation phase transforms the raw supermarket transaction data into a structured
and fully numeric dataset suitable for regression modeling. All preprocessing steps described
in this section are directly derived from the provenance graph by querying activities that are
part of the Data Preparation phase (\texttt{sc:isPartOf :data\_preparation\_phase}).

\paragraph{Documented preprocessing steps.}
The following preprocessing activities were executed and logged in the provenance graph as
individual \texttt{prov:Activity} instances, together with their associated comments and
execution timestamps:

\begin{itemize}
\item Outlier handling was executed based on the Data Understanding decision.
The detected outliers correspond to valid high-value supermarket transactions and were
therefore retained. No rows were removed in this step.
\item Feature selection was performed based on Data Understanding results.
Non-predictive identifiers such ar Invoice ID, constant attributes which refers to "gross margin percentage", 
and redundant monetary features like "Tax 5\%" and "cogs" were removed to reduce dimensionality and similarly corelated variables.
\item Raw Date and Time attributes were transformed into interpretable temporal
features (month, day of week, hour and minute) to capture seasonal, weekly, and
intra-day effects. The original Date and Time columns were removed to avoid
incorrect numerical interpretation.
\item Categorical attributes were transformed using One-Hot Encoding. The reason for it is to avoid introducing artificial ordering or distance assumptions, because this kind of categorical variables does not have ordinal relationship.This tries to ensure correct interpretation by regression models and improves model robustness. One-hot encoding was applied with setting parameter to True, which results in implicit reference categories for each categorical variable. For example, Gender\_Female and CustomerType\_Member are represented as baseline (value 0) and they are not explicitly stored as separate variables.
\item Numerical features were standardized using StandardScaler. This is done in order to to ensure comparable
feature scales and stable regression behavior. One-hot encoded categorical
features and the target variable were excluded from scaling.
\item \textbf{3b Preprocessing Steps Considered but Not Applied} 

Several preprocessing options were evaluated but deliberately not applied.
\begin{enumerate}
    \item Outlier removal was considered based on statistical calculation. However, identified outliers correspond to valid high-value transactions and were retained.
    \item Label (ordinal) encoding of categorical attributes was rejected to avoid introducing artificial ordering into nominal variables.
    \item Binning of the target variable (Rating) was not applied, because analysis is formulated as regression task and binning would reduce information granularity.
    \item The raw Date and Time attributes were not used directly, because regression models cannot meaningfully interpret timestamps without transformation.
\end{enumerate}


\item \textbf{3c Derived Attributes}

Derived attributes were introduced to improve interpretability and predictive performance.
\begin{enumerate}
    \item Nominal categorical variables were transformed using one-hot encoding to avoid artificial ordering assumptions.
    \item Temporal information was extracted from Date and Time attributes by in a way that month, day of week, hour and minute features were derived to capture seasonal, weekly, and intra-day effects.
\end{enumerate}
More complex derived attributes were considered but not applied to avoid not necessary feature expansion.

\item \textbf{ 3d External Data Sources and Attributes}

Additional external data sources could potentially improve the prediction of customer ratings.
Examples include promotional calendars, public holidays, local events,
or regional economic indicators. This type of data could help to explain contextual and temporal variations
in customer satisfaction. These data sources were not integrated because of limited availability.
\end{itemize}

Each activity explicitly records its input and output datasets using provenance relations
such as \texttt{prov:used}, \texttt{prov:wasGeneratedBy}, and \texttt{prov:wasDerivedFrom},
ensuring full traceability of all data transformations.

\paragraph{Final prepared dataset.}
The outcome of the Data Preparation phase is the dataset
\texttt{:prepared\_data} (label: \emph{Prepared Dataset for Modeling}), which is documented in the provenance
graph as a \texttt{prov:Entity}. The dataset description retrieved from the graph is as follows:

\begin{quote}
This dataset represents final output of the Data Preparation phase.
It includes selected numerical features, one-hot encoded categorical variables,
derived temporal attributes (month, day of week, hour, minute), and standardized numerical
values. This dataset is fully numeric, reproducible, and ready to be used as input
for regression modeling.
\end{quote}



%% --- 4. Modeling ---
\section{Modeling}

\subsection{Hyperparameter Configuration}
The model was trained using the following hyperparameter settings:

\begin{table}[h]
  \caption{Hyperparameter Settings}
  \label{tab:hyperparams}
  \begin{tabular}{lp{0.4\linewidth}l}
    \toprule
    \textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
    \midrule
    
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Training Run}
A training run was executed with the following characteristics:
\begin{itemize}
    \item \textbf{Algorithm:} 
    \item \textbf{Start Time:} 
    \item \textbf{End Time:} 
    \item \textbf{Result:}  = 
\end{itemize}

%% --- 5. Evaluation ---
\section{Evaluation}

%% --- 6. Deployment ---
\section{Deployment}

\section{Conclusion}

\end{document}
