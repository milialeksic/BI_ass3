\documentclass[sigconf]{acmart}

\AtBeginDocument{ \providecommand\BibTeX{ Bib\TeX } }
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[BI 2025]{Business Intelligence}{-}{-}

\begin{document}

\title{BI2025 Experiment Report - Group 008}
%% ---Authors: Dynamically added ---


\begin{abstract}
  This report documents the machine learning experiment for Group 008, following the CRISP-DM process model.
\end{abstract}

\ccsdesc[500]{Computing methodologies~Machine learning}
\keywords{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}

\maketitle

%% --- 1. Business Understanding ---
\section{Business Understanding}

\subsection{Data Source and Scenario}
The dataset is the Supermarket Sales dataset. It contains transaction-level data including
branch, customer type, gender, product line, payment method, unit price, quantity, tax, total sales, date, time,
and a customer satisfaction rating (4 to 10). Scenario: The supermarket chain wants to monitor customer satisfaction
and predict the rating a customer is likely to give. The business particularly wants to detect low predicted ratings
early to improve service quality and customer experience.

\subsection{Business Objectives}
The primary business objectives are:
1. Understand which factors affects customer satisfaction.
2. Predict customer ratings based on available information.
3. Detect scenarios likely to produce low ratings and support decisions to improve service.

\subsection{Business Success Criteria}
The project will be considered a business success if:
1. The model predicts customer ratings with useful accuracy (e.g., MAE sufficiently low to distinguish low vs. high satisfaction cases).
2. The system can identify transactions likely to result in low ratings, which would enable proactive service improvements.
3. The analysis provides useful insights into factors associated with customer satisfaction.

\subsection{Data Mining Goals}
The data mining goals are:
1. Train a regression model to predict the continuous customer rating variable.
2. Understand feature importance to identify which attributes most influence satisfaction.
3. Optionally compare multiple ML models.
4. Ensure full reproducibility of all experiments.

\subsection{Data Mining Success Criteria}
Data mining will be considered successful if:
1. The model achieves meaningful and realistic MAE threshold (e.g., ≤ 1.0).
2. The model is interpretable, allowing business users to understand what determines the low satisfaction.
3. The entire workflow is reproducible and computationally efficient.

\subsection{AI Risk Aspects}
Relevant AI risks include:
1. Customer type (Member/Normal) could result in unfair treatment if misused.
2. Using model predictions to treat customers differently could be unethical or discriminatory.
3. Predictions must be used only as decision support and not as a mechanism for unequal service.


%% --- 2. Data Understanding ---
\section{Data Understanding}
\textbf{Dataset Description:} Transaction-level supermarket sales data including customer attributes, product information, sales amounts, timestamps, and customer ratings.

The following features were identified in the dataset:

\begin{table}[h]
  \caption{Raw Data Features}
  \label{tab:features}
  \begin{tabular}{lp{0.2\linewidth}p{0.4\linewidth}}
    \toprule
    \textbf{Feature Name} & \textbf{Data Type} & \textbf{Description} \\
    \midrule
    Branch & string> & Supermarket branch where the transaction occurred. \\
    City & string> & City where the branch is located. \\
    Customer type & string> & Customer classification: Member or Normal. \\
    Date & date> & Date of transaction. \\
    Gender & string> & Gender of the customer. \\
    Invoice ID & string> & Unique identifier for each transaction. \\
    Payment & string> & Payment method used in the transaction. \\
    Product line & string> & Category of product purchased. \\
    Quantity & integer> & Number of items purchased. \\
    Rating & float> & Customer satisfaction rating on a scale from 4 to 10. \\
    Sales & float> & Total amount paid including tax. \\
    Tax 5\% & float> & 5 percent tax applied to the purchase. \\
    Time & string> & Time of transaction. \\
    Unit price & float> & Price per unit of product. \\
    cogs & float> & Cost of goods sold (pre-tax). \\
    gross income & float> & Gross income from the transaction. \\
    gross margin percentage & float> & Gross margin percentage (constant in dataset). \\
    \bottomrule
  \end{tabular}
\end{table}


\subsectionData Understanding – Outlier Detection
Potential outliers were identified using Z-score based approach for numerical attributes.
Treshold of 2.2 is used as cut-off value.
The analysis shows that outliers were not detected for Unit price, Quantity, or Rating.

Outliers were identified for Tax 5\%, Sales, cogs, and gross income. These outliers occur
at the same transaction indices across these attributes, which is expected because
these monetary variables are deterministically related (Sales = cogs + Tax 5\% and
gross income is proportional to cogs).

The detected outliers correspond to unusually big purchase rather than
error values. This indicates the presence of high-value transactions rather than
data quality issues.

\subsectionData Understanding – Outlier Assessment
The outlier report was inspected to check the authenticity of the detected values.
The identified outliers occur only in monetary attributes and correspond to
large but realistic purchase amounts.

Because these values represent valid high-spending transactions and no outliers were
detected for the target variable (Rating), no records are removed during the Data
Understanding phase. The presence of these values reflects natural variability in
customer spending rather than data errors.

Any potential handling of extreme values (e.g. capping or transformation) is moved
to the Data Preparation phase if required by the modeling approach.

\textbf{Outlier Decision:} No outliers removed during Data Understanding; decision deferred to Data Preparation.

\subsectionStatistical Properties and Correlations
Statistical properties and correlations were computed for all numeric attributes of
Supermarket Sales dataset. The dataset contains 1,000 complete records with no missing
values.

Descriptive statistics show that unit prices range is from approximately 10 to 100, with a
mean = 55.7. Customers purchase on usually 5 to 6 items per transaction, with
quantities which have range from 1 to 10. Sales amounts and cost-related attributes 
The gross margin percentage is constant across all records (4.76\%),
which results in zero variance.

Customer ratings range from 4 to 10, with mean  approx. 6.97 and standard
deviation of 1.72, which indicates moderate variability in customer satisfaction.

Pearson correlation analysis with regression target 'Rating' reveals that none of
numeric attributes have a strong linear relationship with target variable. All
observed correlations are close to zero (|r| < 0.04). Money related attributes such as Sales,
cogs, Tax 5\%, and gross income show similar correlations with target, which confirms 
their relationships. The constant gross margin percentage shows no defined
correlation.

These results indicate that customer satisfaction is not primarily driven by purchase
volume or price-related factors only. Instead, categorical attributes, for example, branch,
product line, payment method and potential non-linear effects are likely to play a more
important role in predicting customer ratings.


\begin{table}[h]
\centering
\begin{tabular}{lr}
\hline
\textbf{Variable} & \textbf{Correlation} \\
\hline
Rating & 1.000000 \\
Unit price & -0.008778 \\
Quantity & -0.015815 \\
cogs & -0.036442 \\
Tax 5\% & -0.036442 \\
gross income & -0.036442 \\
Sales & -0.036442 \\
gross margin percentage & nan \\
\hline
\end{tabular}
\caption{Correlation of numerical attributes with Rating}
\end{table}



\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lrrrr}
\hline
\textbf{Variable} & \textbf{count} & \textbf{mean} & \textbf{std} & \textbf{median} \\
\hline
Unit price & 1000.0 & 55.672130 & 26.494628 & 55.230000 \\
Quantity & 1000.0 & 5.510000 & 2.923431 & 5.000000 \\
Tax 5\% & 1000.0 & 15.379369 & 11.708825 & 12.088000 \\
Sales & 1000.0 & 322.966749 & 245.885335 & 253.848000 \\
cogs & 1000.0 & 307.587380 & 234.176510 & 241.760000 \\
gross margin percentage & 1000.0 & 4.761905 & 0.000000 & 4.761905 \\
gross income & 1000.0 & 15.379369 & 11.708825 & 12.088000 \\
Rating & 1000.0 & 6.972700 & 1.718580 & 7.000000 \\
\hline
\end{tabular}
\caption{Count, mean, standard deviation, and median of numerical attributes}
\end{table}



\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lrrrr}
\hline
\textbf{Variable} & \textbf{min} & \textbf{25\%} & \textbf{75\%} & \textbf{max} \\
\hline
Unit price & 10.080000 & 32.875000 & 77.935000 & 99.960000 \\
Quantity & 1.000000 & 3.000000 & 8.000000 & 10.000000 \\
Tax 5\% & 0.508500 & 5.924875 & 22.445250 & 49.650000 \\
Sales & 10.678500 & 124.422375 & 471.350250 & 1042.650000 \\
cogs & 10.170000 & 118.497500 & 448.905000 & 993.000000 \\
gross margin percentage & 4.761905 & 4.761905 & 4.761905 & 4.761905 \\
gross income & 0.508500 & 5.924875 & 22.445250 & 49.650000 \\
Rating & 4.000000 & 5.500000 & 8.500000 & 10.000000 \\
\hline
\end{tabular}
\caption{Minimum, quartiles, and maximum of numerical attributes}
\end{table}








%% --- 3. Data Preparation ---
\section{Data Preparation}
\subsection{Data Cleaning}

The Data Preparation phase transforms the raw supermarket transaction data into a structured
and fully numeric dataset suitable for regression modeling. All preprocessing steps described
in this section are directly derived from the provenance graph by querying activities that are
part of the Data Preparation phase (\texttt{sc:isPartOf :data\_preparation\_phase}).

\paragraph{Documented preprocessing steps.}
The following preprocessing activities were executed and logged in the provenance graph as
individual \texttt{prov:Activity} instances, together with their associated comments and
execution timestamps:

\begin{itemize}
\item Outlier handling was executed based on the Data Understanding decision.
The detected outliers correspond to valid high-value supermarket transactions and were
therefore retained. No rows were removed in this step.
\item Feature selection was performed based on Data Understanding results.
Non-predictive identifiers such ar Invoice ID, constant attributes which refers to "gross margin percentage", 
and redundant monetary features like "Tax 5\%" and "cogs" were removed to reduce dimensionality and similarly corelated variables.
\item Raw Date and Time attributes were transformed into interpretable temporal
features (month, day of week, hour and minute) to capture seasonal, weekly, and
intra-day effects. The original Date and Time columns were removed to avoid
incorrect numerical interpretation.
\item Categorical attributes were transformed using One-Hot Encoding. The reason for it is to avoid introducing artificial ordering or distance assumptions, because this kind of categorical variables does not have ordinal relationship.This tries to ensure correct interpretation by regression models and improves model robustness. One-hot encoding was applied with setting parameter to True, which results in implicit reference categories for each categorical variable. For example, Gender\_Female and CustomerType\_Member are represented as baseline (value 0) and they are not explicitly stored as separate variables.
\item Numerical features were standardized using StandardScaler. This is done in order to to ensure comparable
feature scales and stable regression behavior. One-hot encoded categorical
features and the target variable were excluded from scaling.
\item 3b Preprocessing Steps Considered but Not Applied
\item 3c Derived Attributes
\item 3d External Data Sources and Attributes
\end{itemize}



Each activity explicitly records its input and output datasets using provenance relations
such as \texttt{prov:used}, \texttt{prov:wasGeneratedBy}, and \texttt{prov:wasDerivedFrom},
ensuring full traceability of all data transformations.

\paragraph{Final prepared dataset.}
The outcome of the Data Preparation phase is the dataset
\texttt{:prepared\_data} (label: \emph{Prepared Dataset for Modeling}), which is documented in the provenance
graph as a \texttt{prov:Entity}. The dataset description retrieved from the graph is as follows:

\begin{quote}
This dataset represents final output of the Data Preparation phase.
It includes selected numerical features, one-hot encoded categorical variables,
derived temporal attributes (month, day of week, hour, minute), and standardized numerical
values. This dataset is fully numeric, reproducible, and ready to be used as input
for regression modeling.
\end{quote}



%% --- 4. Modeling ---
\section{Modeling}

\subsection{Hyperparameter Configuration}
The model was trained using the following hyperparameter settings:

\begin{table}[h]
  \caption{Hyperparameter Settings}
  \label{tab:hyperparams}
  \begin{tabular}{lp{0.4\linewidth}l}
    \toprule
    \textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
    \midrule
    Learning Rate & ... & 1.23 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Training Run}
A training run was executed with the following characteristics:
\begin{itemize}
    \item \textbf{Algorithm:} Random Forest Algorithm
    \item \textbf{Start Time:} 2025-12-15 20:45:27
    \item \textbf{End Time:} 2025-12-15 20:45:27
    \item \textbf{Result:} R-squared Score = 1.2300
\end{itemize}

%% --- 5. Evaluation ---
\section{Evaluation}

%% --- 6. Deployment ---
\section{Deployment}

\section{Conclusion}

\end{document}
